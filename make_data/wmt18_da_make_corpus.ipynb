{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DATA_HOME = '/ahc/work3/kosuke-t/data/'\n",
    "#DATA_HOME = sys.argv[-1]\n",
    "\n",
    "DA_HOME = os.path.join(DATA_HOME, 'WMT/newstest2018-humaneval/analysis')\n",
    "DARR_HOME = os.path.join(DATA_HOME, 'WMT/wmt18-metrics-task-package/manual-evaluation/RR-seglevel.csv')\n",
    "SRC_HOME = os.path.join(DATA_HOME, 'WMT/wmt18-metrics-task-package/source-system-outputs/wmt18-submitted-data/txt/sources')\n",
    "REF_HOME = os.path.join(DATA_HOME, 'WMT/wmt18-metrics-task-package/source-system-outputs/wmt18-submitted-data/txt/references')\n",
    "HYP_HOME = os.path.join(DATA_HOME, 'WMT/wmt18-metrics-task-package/source-system-outputs/wmt18-submitted-data/txt/system-outputs/newstest2018')\n",
    "SAVE_PATH_DARR = os.path.join(DATA_HOME, 'WMT/wmt18_darr.pkl')\n",
    "SAVE_PATH_DA_GOOD_REDUP = os.path.join(DATA_HOME, 'WMT/wmt18_da_good_redup.pkl')\n",
    "SAVE_PATH_DA_SEG = os.path.join(DATA_HOME, 'WMT/wmt18_da_seg.pkl')\n",
    "\n",
    "SAVE_SRC_TRAIN = os.path.join(DATA_HOME,'SRHDA/WMT15_18_DA/train.src')\n",
    "SAVE_REF_TRAIN = os.path.join(DATA_HOME,'SRHDA/WMT15_18_DA/train.ref')\n",
    "SAVE_HYP_TRAIN = os.path.join(DATA_HOME,'SRHDA/WMT15_18_DA/train.hyp')\n",
    "SAVE_LABEL_TRAIN = os.path.join(DATA_HOME,'SRHDA/WMT15_18_DA/train.label')\n",
    "SAVE_SRC_VALID = os.path.join(DATA_HOME,'SRHDA/WMT15_18_DA/valid.src')\n",
    "SAVE_REF_VALID = os.path.join(DATA_HOME,'SRHDA/WMT15_18_DA/valid.ref')\n",
    "SAVE_HYP_VALID = os.path.join(DATA_HOME,'SRHDA/WMT15_18_DA/valid.hyp')\n",
    "SAVE_LABEL_VALID = os.path.join(DATA_HOME,'SRHDA/WMT15_18_DA/valid.label')\n",
    "SAVE_SRC_TEST = os.path.join(DATA_HOME,'SRHDA/WMT15_18_DA/test.src')\n",
    "SAVE_REF_TEST = os.path.join(DATA_HOME,'SRHDA/WMT15_18_DA/test.ref')\n",
    "SAVE_HYP_TEST = os.path.join(DATA_HOME,'SRHDA/WMT15_18_DA/test.hyp')\n",
    "SAVE_LABEL_TEST = os.path.join(DATA_HOME,'SRHDA/WMT15_18_DA/test.label')\n",
    "\n",
    "langs = ['cs-en', 'de-en', 'et-en', 'fi-en', 'ru-en', 'tr-en', 'zh-en', \n",
    "         'en-cs', 'en-de', 'en-et', 'en-fi', 'en-ru', 'en-tr', 'en-zh']\n",
    "\n",
    "# systems = {'cs-en':['CUNI-Transformer.5560', \n",
    "#                     'online-A.0', \n",
    "#                     'online-B.0', \n",
    "#                     'online-G.0', \n",
    "#                     'uedin.5561'], \n",
    "#            'de-en':[], \n",
    "#            'et-en':[], \n",
    "#            'fi-en':[], \n",
    "#            'ru-en':[], \n",
    "#            'tr-en':[], \n",
    "#            'zh-en':[], \n",
    "#            'en-cs':[], \n",
    "#            'en-de':[], \n",
    "#            'en-et':[], \n",
    "#            'en-fi':[], \n",
    "#            'en-ru':[], \n",
    "#            'en-tr':[], \n",
    "#            'en-zh':[]}\n",
    "\n",
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "import csv\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from  tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    data = []\n",
    "    with open(filename, mode='r', encoding='utf-8') as r:\n",
    "        data = r.read().split(os.linesep)\n",
    "        if data[-1] == '':\n",
    "            data.pop(-1)\n",
    "    return data\n",
    "\n",
    "SRC_files = {lang:load_file(os.path.join(SRC_HOME, 'newstest2018-{0}{1}-src.{0}'.format(lang.split('-')[0], lang.split('-')[1])))  for lang in langs}\n",
    "REF_files = {lang:load_file(os.path.join(REF_HOME, 'newstest2018-{0}{1}-ref.{1}'.format(lang.split('-')[0], lang.split('-')[1]))) for lang in langs}\n",
    "HYP_files = {lang:{} for lang in langs}\n",
    "\n",
    "for lang in langs:\n",
    "    for fname in os.listdir(os.path.join(HYP_HOME, lang)):\n",
    "        if not fname.startswith('newstest2018'):\n",
    "            continue\n",
    "        # extract system id from fname\n",
    "        system_id = copy.deepcopy(fname).replace('newstest2018.', '').replace('.{}'.format(lang), '')\n",
    "        # add\n",
    "        HYP_files[lang][system_id] = load_file(os.path.join(os.path.join(HYP_HOME, lang), fname))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â†“DARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /ahc/work3/kosuke-t/data/WMT/wmt18_darr.pkl\n"
     ]
    }
   ],
   "source": [
    "DArr = load_file(DARR_HOME)\n",
    "corpus = []\n",
    "for idx, da_data in enumerate(DArr):\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    lang = da_data.split(' ')[0]\n",
    "    sid = int(da_data.split(' ')[2])\n",
    "    better_sys = da_data.split(' ')[3]\n",
    "    worse_sys = da_data.split(' ')[4]\n",
    "    corpus.append({'lang': lang, \n",
    "                   'sid':sid,\n",
    "                   'year':18,\n",
    "                   'src': SRC_files[lang][sid-1], \n",
    "                   'ref': REF_files[lang][sid-1], \n",
    "                   'hyp1': HYP_files[lang][better_sys][sid-1], \n",
    "                   'hyp2': HYP_files[lang][worse_sys][sid-1], \n",
    "                   'better':'hyp1'})\n",
    "print('saving {}'.format(SAVE_PATH_DARR))\n",
    "with open(SAVE_PATH_DARR, mode='wb') as w:\n",
    "    pickle.dump(corpus, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DA for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_good_redup = {lang: os.path.join(DA_HOME, 'ad-{}-good-stnd-redup.csv'.format(lang.replace('-', ''))) for lang in langs}\n",
    "#filename_good_redup = {lang: os.path.join(DA_HOME, 'ad-{}-good-stnd.csv'.format(lang.replace('-', ''))) for lang in langs}\n",
    "filename_seg_scores = {lang: os.path.join(DA_HOME, 'ad-seg-scores-{}.csv'.format(lang)) for lang in langs}\n",
    "\n",
    "DA_data_good_redup = {lang: load_file(f) for lang, f in filename_good_redup.items()}\n",
    "DA_data_seg_scores = {lang: load_file(f) for lang, f in filename_seg_scores.items()}\n",
    "\n",
    "def make_corpus_good_stnd_redup(langs, DA_data):\n",
    "    corpus = []\n",
    "    type_set = set()\n",
    "    for lang in langs:\n",
    "        for idx, row in enumerate(DA_data[lang]):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "\n",
    "            type_id = row.split('\\t')[8]\n",
    "            score = float(row.split('\\t')[-2])\n",
    "            sid = int(row.split('\\t')[9])\n",
    "            system_id = row.split('\\t')[6]\n",
    "\n",
    "            type_set.add(type_id)\n",
    "\n",
    "            if type_id != 'SYSTEM':\n",
    "                continue\n",
    "            if '+' in system_id:\n",
    "                continue\n",
    "            corpus.append({'lang':lang,\n",
    "                           'sid':sid,\n",
    "                           'year':18,\n",
    "                           'src':SRC_files[lang][sid-1],\n",
    "                           'ref':REF_files[lang][sid-1],\n",
    "                           'hyp':HYP_files[lang][system_id][sid-1],\n",
    "                           'label':score})\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def make_corpus_seg_scores(langs, DA_data):\n",
    "    corpus = []\n",
    "    for lang in langs:\n",
    "        for idx, row in enumerate(DA_data[lang]):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            system_id = row.split(' ')[0]\n",
    "            sid = int(row.split(' ')[1])\n",
    "            score = float(row.split(' ')[3])\n",
    "            n = int(row.split(' ')[4])\n",
    "            if system_id == 'HUMAN':\n",
    "                continue\n",
    "            \n",
    "            corpus.append({'lang':lang,\n",
    "                           'sid':sid,\n",
    "                           'year':18,\n",
    "                           'n':n,\n",
    "                           'src':SRC_files[lang][sid-1],\n",
    "                           'ref':REF_files[lang][sid-1],\n",
    "                           'hyp':HYP_files[lang][system_id][sid-1],\n",
    "                           'label':score})\n",
    "    return corpus\n",
    "\n",
    "corpus_good_redup = make_corpus_good_stnd_redup(langs, DA_data_good_redup)\n",
    "corpus_seg_scores = make_corpus_seg_scores(langs, DA_data_seg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_langs = {}\n",
    "for corpus in corpus_seg_scores:\n",
    "    lang = corpus['lang']\n",
    "    n = corpus['n']\n",
    "    if lang not in times_langs:\n",
    "        times_langs[lang] = {}\n",
    "        times_langs[lang][n] = 1\n",
    "    else:\n",
    "        if n not in times_langs[lang]:\n",
    "            times_langs[lang][n] = 1\n",
    "        else:\n",
    "            times_langs[lang][n] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs-en': {2: 2393, 1: 6607, 3: 272},\n",
       " 'de-en': {2: 9450, 3: 2014, 6: 9, 1: 22480, 4: 232, 5: 13},\n",
       " 'et-en': {1: 16705, 2: 5083, 3: 588, 4: 57, 5: 1},\n",
       " 'fi-en': {3: 84, 2: 2624, 1: 13356, 4: 3},\n",
       " 'ru-en': {2: 2814, 1: 11255, 3: 256, 4: 15},\n",
       " 'tr-en': {1: 8858, 5: 35, 2: 3887, 3: 408, 4: 279, 6: 4},\n",
       " 'zh-en': {1: 24007, 3: 82, 2: 4323, 4: 5},\n",
       " 'en-cs': {1: 6119, 2: 1601, 3: 253},\n",
       " 'en-de': {1: 10369, 2: 1471, 3: 127, 4: 13, 5: 2},\n",
       " 'en-et': {2: 1479, 3: 44, 1: 12706, 4: 1},\n",
       " 'en-fi': {2: 1119, 1: 7571, 3: 62},\n",
       " 'en-ru': {1: 10862, 3: 1552, 2: 5607, 4: 247, 5: 43, 6: 7},\n",
       " 'en-tr': {1: 3182, 3: 6, 2: 222},\n",
       " 'en-zh': {1: 19483, 2: 5336, 3: 650, 4: 74, 5: 2}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_outlier(corpus):\n",
    "#     human_scores = {}\n",
    "#     mean = {}\n",
    "#     MAP = {}\n",
    "#     z = {}\n",
    "#     for c in corpus:\n",
    "#         lang = c['lang']\n",
    "#         if lang not in human_scores:\n",
    "#             human_scores[lang] = [c['label']]\n",
    "#         else:\n",
    "#             human_scores[lang].append(c['label'])\n",
    "    \n",
    "#     for lang in human_scores.keys():\n",
    "#         mean[lang] = np.mean(human_scores[lang]) \n",
    "#         MAP[lang] = 1.483 * np.mean([s - mean[lang] for s in human_scores[lang]])\n",
    "#         z[lang] = ([(s - mean[lang])/MAP[lang] for s in human_scores[lang]])\n",
    "        \n",
    "#     return human_scores, mean, MAP, z\n",
    "\n",
    "# human_scores, mean, MAP, z = remove_outlier(corpus_seg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good redup\n",
      "-- corpus size for each language pair ---\n",
      "cs-en has 11154 instances\n",
      "de-en has 43845 instances\n",
      "et-en has 25625 instances\n",
      "fi-en has 16589 instances\n",
      "ru-en has 15678 instances\n",
      "tr-en has 16921 instances\n",
      "zh-en has 28819 instances\n",
      "en-cs has 9781 instances\n",
      "en-de has 13208 instances\n",
      "en-et has 15759 instances\n",
      "en-fi has 9708 instances\n",
      "en-ru has 25641 instances\n",
      "en-tr has 3491 instances\n",
      "en-zh has 29168 instances\n",
      "Total : 265387\n",
      "To En Total : 158631\n",
      "\n",
      "seg scores\n",
      "-- corpus size for each language pair ---\n",
      "cs-en has 9272 instances\n",
      "de-en has 34198 instances\n",
      "et-en has 22434 instances\n",
      "fi-en has 16067 instances\n",
      "ru-en has 14340 instances\n",
      "tr-en has 13471 instances\n",
      "zh-en has 28417 instances\n",
      "en-cs has 7973 instances\n",
      "en-de has 11982 instances\n",
      "en-et has 14230 instances\n",
      "en-fi has 8752 instances\n",
      "en-ru has 18318 instances\n",
      "en-tr has 3410 instances\n",
      "en-zh has 25545 instances\n",
      "Total : 228409\n",
      "To En Total : 138199\n",
      "\n",
      "seg scores over n==2\n",
      "-- corpus size for each language pair ---\n",
      "cs-en has 2665 instances\n",
      "de-en has 11718 instances\n",
      "et-en has 5729 instances\n",
      "fi-en has 2711 instances\n",
      "ru-en has 3085 instances\n",
      "tr-en has 4613 instances\n",
      "zh-en has 4410 instances\n",
      "en-cs has 1854 instances\n",
      "en-de has 1613 instances\n",
      "en-et has 1524 instances\n",
      "en-fi has 1181 instances\n",
      "en-ru has 7456 instances\n",
      "en-tr has 228 instances\n",
      "en-zh has 6062 instances\n",
      "Total : 54849\n",
      "To En Total : 34931\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('good redup')\n",
    "print('-- corpus size for each language pair ---')\n",
    "lang_count = {lang:0 for lang in langs}\n",
    "for corpus in corpus_good_redup:\n",
    "    lang = corpus['lang']\n",
    "    lang_count[lang] += 1\n",
    "total = 0\n",
    "toen_total = 0\n",
    "for lang in langs:\n",
    "    total += lang_count[lang]\n",
    "    if lang.endswith('en'):\n",
    "        toen_total += lang_count[lang]\n",
    "    print('{} has {} instances'.format(lang, lang_count[lang]))\n",
    "print('{} : {}'.format('Total', total))\n",
    "print('{} : {}'.format('To En Total', toen_total))\n",
    "print()\n",
    "\n",
    "print('seg scores')\n",
    "print('-- corpus size for each language pair ---')\n",
    "lang_count = {lang:0 for lang in langs}\n",
    "for corpus in corpus_seg_scores:\n",
    "    lang = corpus['lang']\n",
    "    lang_count[lang] += 1\n",
    "total = 0\n",
    "toen_total = 0\n",
    "for lang in langs:\n",
    "    total += lang_count[lang]\n",
    "    if lang.endswith('en'):\n",
    "        toen_total += lang_count[lang]\n",
    "    print('{} has {} instances'.format(lang, lang_count[lang]))\n",
    "print('{} : {}'.format('Total', total))\n",
    "print('{} : {}'.format('To En Total', toen_total))\n",
    "print()\n",
    "\n",
    "print('seg scores over n==2')\n",
    "print('-- corpus size for each language pair ---')\n",
    "lang_count = {lang:0 for lang in langs}\n",
    "for corpus in corpus_seg_scores:\n",
    "    lang = corpus['lang']\n",
    "    if corpus['n'] == 1:\n",
    "        continue\n",
    "    lang_count[lang] += 1\n",
    "total = 0\n",
    "toen_total = 0\n",
    "for lang in langs:\n",
    "    total += lang_count[lang]\n",
    "    if lang.endswith('en'):\n",
    "        toen_total += lang_count[lang]\n",
    "    print('{} has {} instances'.format(lang, lang_count[lang]))\n",
    "print('{} : {}'.format('Total', total))\n",
    "print('{} : {}'.format('To En Total', toen_total))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /ahc/work3/kosuke-t/data/WMT/wmt18_da_good_redup.pkl\n",
      "saving /ahc/work3/kosuke-t/data/WMT/wmt18_da_seg.pkl\n"
     ]
    }
   ],
   "source": [
    "print('saving {}'.format(SAVE_PATH_DA_GOOD_REDUP))\n",
    "with open(SAVE_PATH_DA_GOOD_REDUP, mode='wb') as w:\n",
    "    pickle.dump(corpus_good_redup, w)\n",
    "    \n",
    "print('saving {}'.format(SAVE_PATH_DA_SEG))\n",
    "with open(SAVE_PATH_DA_SEG, mode='wb') as w:\n",
    "    pickle.dump(corpus_seg_scores, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        print('{} does not exist'.format(filename))\n",
    "        exit(-2)\n",
    "    data = None\n",
    "    with open(filename, mode='rb') as r:\n",
    "        data = pickle.load(r)\n",
    "    return data\n",
    "\n",
    "# return True when duplicated\n",
    "def dup_check(train_data, valid_data):\n",
    "    flag = False\n",
    "    duplicate_dic = {}\n",
    "    dup_index = []\n",
    "    for i, val in enumerate(valid_data):\n",
    "        key = (val['lang'], val['year'], val['sid'])\n",
    "        if key not in duplicate_dic:\n",
    "            duplicate_dic[key] = [i]\n",
    "        else:\n",
    "            duplicate_dic[key].append(i)\n",
    "    for i, trn in enumerate(train_data):\n",
    "        key = (trn['lang'], trn['year'], trn['sid'])\n",
    "        if key in duplicate_dic:\n",
    "            flag = True\n",
    "            dup_index.append({'train':i, 'valid':duplicate_dic[key]})\n",
    "    return flag, dup_index\n",
    "            \n",
    "\n",
    "def split_data(Alldata, ratio, exception_index, duplication=False):\n",
    "    \n",
    "    all_index = [i for i in range(len(Alldata))]\n",
    "    valid_index = random.sample(list(set(all_index)-set(exception_index)), int((len(Alldata)-len(exception_index))*ratio))\n",
    "    train_index = list(set(all_index)-set(valid_index))\n",
    "\n",
    "    train_data = []\n",
    "    valid_data = []\n",
    "    for idx in all_index:\n",
    "        if idx in train_index:\n",
    "            train_data.append(copy.deepcopy(Alldata[idx]))\n",
    "        else:\n",
    "            valid_data.append(copy.deepcopy(Alldata[idx]))\n",
    "    \n",
    "#     if duplication:\n",
    "    return train_data, valid_data\n",
    "    \n",
    "#     count = 1\n",
    "#     duplication_flag, dup_index = dup_check(train_data, valid_data)\n",
    "#     if not duplication_flag:\n",
    "#         while(duplication_flag):\n",
    "#             count += 1\n",
    "#             all_index = [i for i in range(len(Alldata))]\n",
    "#             valid_index = random.sample(all_index, int(len(Alldata)*ratio))\n",
    "#             train_index = list(set(all_index)-set(valid_index))\n",
    "\n",
    "#             train_data = []\n",
    "#             valid_data = []\n",
    "#             for idx in all_index:\n",
    "#                 if idx in train_index:\n",
    "#                     train_data.append(copy.deepcopy(Alldata[idx]))\n",
    "#                 else:\n",
    "#                     valid_data.append(copy.deepcopy(Alldata[idx]))\n",
    "#             duplication_flag, dup_index = dup_check(train_data, valid_data)\n",
    "        \n",
    "# #         if count % 100 == 0:\n",
    "# #             print('trial No.{}'.format(count))\n",
    "    \n",
    "# #     print('Total trial : {}'.format(count))\n",
    "    \n",
    "#     return train_data, valid_data\n",
    "\n",
    "def get_dup_index(Alldata):\n",
    "    exception_index = []\n",
    "    dup_set = {}\n",
    "    for idx, data in enumerate(Alldata):\n",
    "        key = (data['lang'], data['year'], data['sid'])\n",
    "        if key not in dup_set:\n",
    "            dup_set[key] = [idx]\n",
    "        else:\n",
    "            exception_index.extend(dup_set[key])\n",
    "            exception_index.append(idx)\n",
    "    exception_index = sorted(list(set(exception_index)))\n",
    "    return exception_index, dup_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train, valid, test\n",
    "Train:Valid = 9:1 in wmt18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.1\n",
    "\n",
    "SAVE_HOME = os.path.join(DATA_HOME, 'WMT')\n",
    "\n",
    "Alldata15_17 = []\n",
    "Alldata15_17.extend(load_pickle(os.path.join(SAVE_HOME, 'wmt15_da.pkl')))\n",
    "Alldata15_17.extend(load_pickle(os.path.join(SAVE_HOME, 'wmt16_da.pkl')))\n",
    "Alldata15_17.extend(load_pickle(os.path.join(SAVE_HOME, 'wmt17_da_seg.pkl')))\n",
    "Alldata_langs = {}\n",
    "for data in Alldata15_17:\n",
    "    lang = data['lang']\n",
    "    if lang not in Alldata_langs:\n",
    "        Alldata_langs[lang] = []\n",
    "    Alldata_langs[lang].append(data)\n",
    "\n",
    "train_data_langs = {}\n",
    "valid_data_langs = {}\n",
    "for lang in Alldata_langs.keys():\n",
    "#     print('splitting {} data'.format(lang))\n",
    "    exception_index, dup_set = get_dup_index(Alldata_langs[lang])\n",
    "    train_data, valid_data = split_data(Alldata_langs[lang], valid_ratio, exception_index, duplication=False)\n",
    "    train_data_langs[lang] = train_data\n",
    "    valid_data_langs[lang] = valid_data\n",
    "\n",
    "Darr = load_pickle(SAVE_PATH_DARR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = []\n",
    "ref_train = []\n",
    "hyp_train = []\n",
    "label_train = []\n",
    "\n",
    "src_valid = []\n",
    "ref_valid = []\n",
    "hyp_valid = []\n",
    "label_valid = []\n",
    "\n",
    "src_test = []\n",
    "ref_test = []\n",
    "hyp_test = []\n",
    "label_test = []\n",
    "\n",
    "for lang in Alldata_langs.keys():\n",
    "    for tdata in train_data_langs[lang]:\n",
    "        src_train.append('{}\\t{}'.format(tdata['src'], lang))\n",
    "        ref_train.append('{}\\t{}'.format(tdata['ref'], lang))\n",
    "        hyp_train.append('{}\\t{}'.format(tdata['hyp'], lang))\n",
    "        label_train.append('{}\\t{}'.format(tdata['label'], lang))\n",
    "    for vdata in valid_data_langs[lang]:\n",
    "        src_valid.append('{}\\t{}'.format(vdata['src'], lang))\n",
    "        ref_valid.append('{}\\t{}'.format(vdata['ref'], lang))\n",
    "        hyp_valid.append('{}\\t{}'.format(vdata['hyp'], lang))\n",
    "        label_valid.append('{}\\t{}'.format(vdata['label'], lang))   \n",
    "\n",
    "# 'lang': lang, \n",
    "# 'sid':sid,\n",
    "# 'year':18,\n",
    "# 'src': SRC_files[lang][sid-1], \n",
    "# 'ref': REF_files[lang][sid-1], \n",
    "# 'hyp1': HYP_files[lang][better_sys][sid-1], \n",
    "# 'hyp2': HYP_files[lang][worse_sys][sid-1], \n",
    "# 'better':'hyp1'\n",
    "sid = 0\n",
    "for idx, test_data in enumerate(Darr):\n",
    "    sid += 1\n",
    "    src_test.append('{}\\t{}'.format(test_data['src'], test_data['lang']))\n",
    "    ref_test.append('{}\\t{}'.format(test_data['ref'], test_data['lang']))\n",
    "    hyp_test.append('{}\\t{}'.format(test_data['hyp1'], test_data['lang']))\n",
    "    sid += 1\n",
    "    src_test.append('{}\\t{}'.format(test_data['src'], test_data['lang']))\n",
    "    ref_test.append('{}\\t{}'.format(test_data['ref'], test_data['lang']))\n",
    "    hyp_test.append('{}\\t{}'.format(test_data['hyp2'], test_data['lang']))\n",
    "    label_test.append('{}>{}\\t{}'.format(sid-1, sid, test_data['lang']))\n",
    "    label_test.append('{}>{}\\t{}'.format(sid-1, sid, test_data['lang']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeout(filename, obj):\n",
    "    with open(filename, mode='w', encoding='utf-8') as w:\n",
    "        for d in obj:\n",
    "            w.write(d+os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeout(SAVE_SRC_TRAIN, src_train)\n",
    "writeout(SAVE_REF_TRAIN, ref_train)\n",
    "writeout(SAVE_HYP_TRAIN, hyp_train)\n",
    "writeout(SAVE_LABEL_TRAIN, label_train)\n",
    "\n",
    "writeout(SAVE_SRC_VALID, src_valid)\n",
    "writeout(SAVE_REF_VALID, ref_valid)\n",
    "writeout(SAVE_HYP_VALID, hyp_valid)\n",
    "writeout(SAVE_LABEL_VALID, label_valid)\n",
    "\n",
    "writeout(SAVE_SRC_TEST, src_test)\n",
    "writeout(SAVE_REF_TEST, ref_test)\n",
    "writeout(SAVE_HYP_TEST, hyp_test)\n",
    "writeout(SAVE_LABEL_TEST, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326853"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
