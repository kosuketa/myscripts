{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_HOME = '/ahc/work3/kosuke-t/data/'\n",
    "\n",
    "DA_HOME = os.path.join(DATA_HOME, 'WMT/newstest2018-humaneval/analysis')\n",
    "DARR_HOME = os.path.join(DATA_HOME, 'WMT/wmt18-metrics-task-package/manual-evaluation/RR-seglevel.csv')\n",
    "SRC_HOME = os.path.join(DATA_HOME, 'WMT/wmt18-metrics-task-package/source-system-outputs/wmt18-submitted-data/txt/sources')\n",
    "REF_HOME = os.path.join(DATA_HOME, 'WMT/wmt18-metrics-task-package/source-system-outputs/wmt18-submitted-data/txt/references')\n",
    "HYP_HOME = os.path.join(DATA_HOME, 'WMT/wmt18-metrics-task-package/source-system-outputs/wmt18-submitted-data/txt/system-outputs/newstest2018')\n",
    "SAVE_PATH_DARR = os.path.join(DATA_HOME, 'WMT/wmt18_darr.pkl')\n",
    "SAVE_PATH_DA_GOOD_REDUP = os.path.join(DATA_HOME, 'WMT/wmt18_da_good_redup.pkl')\n",
    "SAVE_PATH_DA_SEG = os.path.join(DATA_HOME, 'WMT/wmt18_da_seg.pkl')\n",
    "\n",
    "langs = ['cs-en', 'de-en', 'et-en', 'fi-en', 'ru-en', 'tr-en', 'zh-en', \n",
    "         'en-cs', 'en-de', 'en-et', 'en-fi', 'en-ru', 'en-tr', 'en-zh']\n",
    "\n",
    "# systems = {'cs-en':['CUNI-Transformer.5560', \n",
    "#                     'online-A.0', \n",
    "#                     'online-B.0', \n",
    "#                     'online-G.0', \n",
    "#                     'uedin.5561'], \n",
    "#            'de-en':[], \n",
    "#            'et-en':[], \n",
    "#            'fi-en':[], \n",
    "#            'ru-en':[], \n",
    "#            'tr-en':[], \n",
    "#            'zh-en':[], \n",
    "#            'en-cs':[], \n",
    "#            'en-de':[], \n",
    "#            'en-et':[], \n",
    "#            'en-fi':[], \n",
    "#            'en-ru':[], \n",
    "#            'en-tr':[], \n",
    "#            'en-zh':[]}\n",
    "\n",
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "import csv\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from  tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    data = []\n",
    "    with open(filename, mode='r', encoding='utf-8') as r:\n",
    "        data = r.read().split(os.linesep)\n",
    "        if data[-1] == '':\n",
    "            data.pop(-1)\n",
    "    return data\n",
    "\n",
    "SRC_files = {lang:load_file(os.path.join(SRC_HOME, 'newstest2018-{0}{1}-src.{0}'.format(lang.split('-')[0], lang.split('-')[1])))  for lang in langs}\n",
    "REF_files = {lang:load_file(os.path.join(REF_HOME, 'newstest2018-{0}{1}-ref.{1}'.format(lang.split('-')[0], lang.split('-')[1]))) for lang in langs}\n",
    "HYP_files = {lang:{} for lang in langs}\n",
    "\n",
    "for lang in langs:\n",
    "    for fname in os.listdir(os.path.join(HYP_HOME, lang)):\n",
    "        if not fname.startswith('newstest2018'):\n",
    "            continue\n",
    "        # extract system id from fname\n",
    "        system_id = copy.deepcopy(fname).replace('newstest2018.', '').replace('.{}'.format(lang), '')\n",
    "        # add\n",
    "        HYP_files[lang][system_id] = load_file(os.path.join(os.path.join(HYP_HOME, lang), fname))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â†“DARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /ahc/work3/kosuke-t/data/WMT/wmt18_darr.pkl\n"
     ]
    }
   ],
   "source": [
    "DArr = load_file(DARR_HOME)\n",
    "corpus = []\n",
    "for idx, da_data in enumerate(DArr):\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    lang = da_data.split(' ')[0]\n",
    "    sid = int(da_data.split(' ')[2])\n",
    "    better_sys = da_data.split(' ')[3]\n",
    "    worse_sys = da_data.split(' ')[4]\n",
    "    corpus.append({'lang': lang, \n",
    "                   'src': SRC_files[lang][sid-1], \n",
    "                   'ref': REF_files[lang][sid-1], \n",
    "                   'hyp1': HYP_files[lang][better_sys][sid-1], \n",
    "                   'hyp2': HYP_files[lang][worse_sys][sid-1], \n",
    "                   'better':'hyp1'})\n",
    "print('saving {}'.format(SAVE_PATH_DARR))\n",
    "with open(SAVE_PATH_DARR, mode='wb') as w:\n",
    "    pickle.dump(corpus, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DA for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_good_redup = {lang: os.path.join(DA_HOME, 'ad-{}-good-stnd-redup.csv'.format(lang.replace('-', ''))) for lang in langs}\n",
    "filename_seg_scores = {lang: os.path.join(DA_HOME, 'ad-seg-scores-{}.csv'.format(lang)) for lang in langs}\n",
    "\n",
    "DA_data_good_redup = {lang: load_file(f) for lang, f in filename_good_redup.items()}\n",
    "DA_data_seg_scores = {lang: load_file(f) for lang, f in filename_seg_scores.items()}\n",
    "\n",
    "def make_corpus_good_stnd_redup(langs, DA_data):\n",
    "    corpus = []\n",
    "    type_set = set()\n",
    "    for lang in langs:\n",
    "        for idx, row in enumerate(DA_data[lang]):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "\n",
    "            type_id = row.split('\\t')[8]\n",
    "            score = float(row.split('\\t')[-2])\n",
    "            sid = int(row.split('\\t')[9])\n",
    "            system_id = row.split('\\t')[6]\n",
    "\n",
    "            type_set.add(type_id)\n",
    "\n",
    "            if type_id != 'SYSTEM':\n",
    "                continue\n",
    "\n",
    "            corpus.append({'lang':lang,\n",
    "                           'src':SRC_files[lang][sid-1],\n",
    "                           'ref':REF_files[lang][sid-1],\n",
    "                           'hyp':HYP_files[lang][system_id][sid-1],\n",
    "                           'label':score})\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def make_corpus_seg_scores(langs, DA_data):\n",
    "    corpus = []\n",
    "    for lang in langs:\n",
    "        for idx, row in enumerate(DA_data[lang]):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            system_id = row.split(' ')[0]\n",
    "            sid = int(row.split(' ')[1])\n",
    "            score = float(row.split(' ')[3])\n",
    "            n = int(row.split(' ')[4])\n",
    "            if system_id == 'HUMAN':\n",
    "#                 print(score)\n",
    "                continue\n",
    "            \n",
    "            corpus.append({'lang':lang,\n",
    "                           'src':SRC_files[lang][sid-1],\n",
    "                           'ref':REF_files[lang][sid-1],\n",
    "                           'hyp':HYP_files[lang][system_id][sid-1],\n",
    "                           'label':score})\n",
    "    return corpus\n",
    "\n",
    "corpus_good_redup = make_corpus_good_stnd_redup(langs, DA_data_good_redup)\n",
    "corpus_seg_scores = make_corpus_seg_scores(langs, DA_data_seg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good redup\n",
      "-- corpus size for each language pair ---\n",
      "cs-en has 11154 instances\n",
      "de-en has 43845 instances\n",
      "et-en has 25625 instances\n",
      "fi-en has 16589 instances\n",
      "ru-en has 15678 instances\n",
      "tr-en has 16921 instances\n",
      "zh-en has 28819 instances\n",
      "en-cs has 9781 instances\n",
      "en-de has 13208 instances\n",
      "en-et has 15759 instances\n",
      "en-fi has 9708 instances\n",
      "en-ru has 25641 instances\n",
      "en-tr has 3491 instances\n",
      "en-zh has 29168 instances\n",
      "\n",
      "seg scores\n",
      "-- corpus size for each language pair ---\n",
      "cs-en has 9272 instances\n",
      "de-en has 34198 instances\n",
      "et-en has 22434 instances\n",
      "fi-en has 16067 instances\n",
      "ru-en has 14340 instances\n",
      "tr-en has 13471 instances\n",
      "zh-en has 28417 instances\n",
      "en-cs has 7973 instances\n",
      "en-de has 11982 instances\n",
      "en-et has 14230 instances\n",
      "en-fi has 8752 instances\n",
      "en-ru has 18318 instances\n",
      "en-tr has 3410 instances\n",
      "en-zh has 25545 instances\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('good redup')\n",
    "print('-- corpus size for each language pair ---')\n",
    "lang_count = {lang:0 for lang in langs}\n",
    "for corpus in corpus_good_redup:\n",
    "    lang = corpus['lang']\n",
    "    lang_count[lang] += 1\n",
    "for lang in langs:\n",
    "    print('{} has {} instances'.format(lang, lang_count[lang]))\n",
    "print()\n",
    "\n",
    "print('seg scores')\n",
    "print('-- corpus size for each language pair ---')\n",
    "lang_count = {lang:0 for lang in langs}\n",
    "for corpus in corpus_seg_scores:\n",
    "    lang = corpus['lang']\n",
    "    lang_count[lang] += 1\n",
    "for lang in langs:\n",
    "    print('{} has {} instances'.format(lang, lang_count[lang]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /ahc/work3/kosuke-t/data/WMT/wmt18_da_good_redup.pkl\n",
      "saving /ahc/work3/kosuke-t/data/WMT/wmt18_da_seg.pkl\n"
     ]
    }
   ],
   "source": [
    "print('saving {}'.format(SAVE_PATH_DA_GOOD_REDUP))\n",
    "with open(SAVE_PATH_DA_GOOD_REDUP, mode='wb') as w:\n",
    "    pickle.dump(corpus_good_redup, w)\n",
    "    \n",
    "print('saving {}'.format(SAVE_PATH_DA_SEG))\n",
    "with open(SAVE_PATH_DA_SEG, mode='wb') as w:\n",
    "    pickle.dump(corpus_seg_scores, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
