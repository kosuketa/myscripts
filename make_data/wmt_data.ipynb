{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "DATAHOME = '/home/is/kosuke-t/project_disc/data/SRHDA/15-18/'\n",
    "\n",
    "langs = {'15':['cs-en', 'de-en',          'fi-en',                   'ru-en'],  # 4\n",
    "         '16':['cs-en', 'de-en',          'fi-en',          'ro-en', 'ru-en', 'tr-en'], # 6\n",
    "         '17':['cs-en', 'de-en',          'fi-en', 'lv-en',          'ru-en', 'tr-en', 'zh-en'], # 7\n",
    "         '18':['cs-en', 'de-en', 'et-en', 'fi-en',                   'ru-en', 'tr-en', 'zh-en'],  # 7\n",
    "         '15-17': ['cs-en', 'de-en', 'fi-en', 'ru-en'],\n",
    "         '15-18': ['cs-en', 'de-en', 'fi-en', 'ru-en']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unienc(text):\n",
    "    if not text:\n",
    "        return ''\n",
    "    return (text.encode('utf-8','ignore')).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load17(SRCF, REFF, HYPF, DAF, lang=None):\n",
    "    srcList = []\n",
    "    refList = []\n",
    "    hypList = []\n",
    "    DAList = []\n",
    "    SIDList = []\n",
    "    \n",
    "    DA_sid = []\n",
    "    DA_sys = []\n",
    "    DA_score = [] \n",
    "    srcdata = []\n",
    "    refdata = []\n",
    "    hypdata = {} #key:system, value:list of sentences\n",
    "    \n",
    "    #load SRC\n",
    "    with open(SRCF, mode='r', encoding='utf-8') as r:\n",
    "        srcdata = list(map(unienc, r.read().split('\\n')))\n",
    "    if srcdata[-1] == '':\n",
    "        srcdata.pop(-1)\n",
    "    \n",
    "    #load REF\n",
    "    with open(REFF, mode='r', encoding='utf-8') as r:\n",
    "        refdata = list(map(unienc, r.read().split('\\n')))\n",
    "    if refdata[-1] == '':\n",
    "        refdata.pop(-1)\n",
    "        \n",
    "    #load HYP\n",
    "    for system in HYPF:\n",
    "        with open(HYPF[system], mode='r', encoding='utf-8') as r:\n",
    "            hypdata[system] = list(map(unienc, r.read().split('\\n')))\n",
    "            if hypdata[system][-1] == '':\n",
    "                hypdata[system].pop(-1)\n",
    "        if re.search('CASICT', system) and lang=='zh-en':\n",
    "            hypdata['CASICT-cons.5144'] = hypdata[system]\n",
    "        elif re.search('ROCMT', system) and lang=='zh-en':\n",
    "            hypdata['ROCMT.5167'] = hypdata[system]\n",
    "    #load DA file\n",
    "    f = open(DAF, mode='r')\n",
    "    \n",
    "    # 'SRC', 'TRG', 'HIT', 'N.raw', 'N.z', 'SID', 'SYS', 'RAW.SCR', 'Z.SCR'\n",
    "    flag = 0\n",
    "    for line in f:\n",
    "        if flag == 0:\n",
    "            flag = 1\n",
    "        else:\n",
    "            if line.split()[0] == lang.split('-')[0] and line.split()[1] == lang.split('-')[-1]:\n",
    "                DA_sid.append(int(line.split()[5]))\n",
    "                DA_sys.append(line.split()[6])\n",
    "                if '+' in DA_sys[-1]:\n",
    "                    DA_sys[-1] = DA_sys[-1].split('+')[0]\n",
    "                # if '+' in sys:\n",
    "                #     sys = sys.split('+')\n",
    "                #     print(sysout_list_dict[sys[0]][sent_ID - 1])\n",
    "                #     print(sysout_list_dict[sys[1]][sent_ID - 1])\n",
    "                DA_score.append(line.split()[-1])\n",
    "    f.close()\n",
    "    \n",
    "    for sid, sys, score in zip(DA_sid, DA_sys, DA_score):\n",
    "        if sys in hypdata:\n",
    "            hypList.append(hypdata[sys][sid-1])\n",
    "            srcList.append(srcdata[sid-1])\n",
    "            refList.append(refdata[sid-1])\n",
    "            DAList.append(float(score))\n",
    "            SIDList.append(sid)\n",
    "#             elif DAdic['sys_id'] == 'REFERENCE':\n",
    "#                 hypList.append(refdata[idx])\n",
    "#                 srcList.append(srcdata[idx])\n",
    "#                 refList.append(refdata[idx])\n",
    "#                 DAList.append(mean(DAdic[str(idx)][sys]))\n",
    "        else:\n",
    "            #print('\\\"{}\\\" not found in DA file'.format(sys))\n",
    "            continue\n",
    "        \n",
    "    return [srcList, refList, hypList, DAList, SIDList]\n",
    "\n",
    "def make17():\n",
    "    srcF = {}\n",
    "    refF = {}\n",
    "    hypF = {}\n",
    "    DAF = {}\n",
    "    data = {}\n",
    "    \n",
    "    SYSTEMS17 = {'cs-en':['online-A.0', 'online-B.0', 'PJATK.4760', 'uedin-nmt.4955'], \n",
    "                 'de-en':['C-3MA.4958', 'online-A.0', 'online-G.0', 'TALP-UPC.4830', 'KIT.4951', 'online-B.0', 'RWTH-nmt-ensemble.4920', 'uedin-nmt.4723', 'LIUM-NMT.4733', 'online-F.0', 'SYSTRAN.4846'], \n",
    "                 'fi-en':['apertium-unconstrained.4793', 'online-A.0', 'online-G.0', 'Hunter-MT.4925', 'online-B.0', 'TALP-UPC.4937'], \n",
    "                 'lv-en':['C-3MA.5067', 'online-A.0', 'tilde-c-nmt-smt-hybrid.5051', 'Hunter-MT.5092', 'online-B.0', 'tilde-nc-nmt-smt-hybrid.5050', 'jhu-pbmt.4980', 'PJATK.4740', 'uedin-nmt.5017'], \n",
    "                 'ru-en':['afrl-mitll-opennmt.4896', 'jhu-pbmt.4978', 'online-A.0', 'online-F.0', 'uedin-nmt.4890', 'afrl-mitll-syscomb.4905', 'NRC.4855', 'online-B.0', 'online-G.0'], \n",
    "                 'tr-en':['afrl-mitll-m2w-nr1.4901', 'JAIST.4859', 'LIUM-NMT.4888', 'online-B.0', 'PROMT-SMT.4737', 'afrl-mitll-syscomb.4902', 'jhu-pbmt.4972', 'online-A.0', 'online-G.0', 'uedin-nmt.4931'], \n",
    "                 'zh-en':['afrl-mitll-opennmt.5109', 'NRC.5172', 'online-G.0', 'SogouKnowing-nmt.5171', 'CASICT-DCU-NMT.5144', 'online-A.0', 'Oregon-State-University-S.5173', 'uedin-nmt.5112', 'jhu-nmt.5151', 'online-B.0', 'PROMT-SMT.5125', 'UU-HNMT.5162', 'NMT-Model-Average-Multi-Cards.5099', 'online-F.0', 'ROCMT.5183', 'xmunmt.5160']\n",
    "                }\n",
    "    DATAHOME = '/project/nakamura-lab08/Work/kosuke-t/data'\n",
    "    for lang in langs['17']:\n",
    "        lang2 = re.sub('-', '', lang)\n",
    "        langSRC = lang[:2]\n",
    "        langREF = lang[-2:]\n",
    "        srcF[lang] = os.path.join(DATAHOME, 'wmt17-submitted-data/txt/sources/newstest2017-{}-src.{}'.format(lang2, langSRC))\n",
    "        refF[lang] = os.path.join(DATAHOME, 'wmt17-submitted-data/txt/references/newstest2017-{}-ref.{}'.format(lang2, langREF))\n",
    "        hypF[lang] = {}\n",
    "        for system in SYSTEMS17[lang]:\n",
    "            hypF[lang][system] = os.path.join(DATAHOME, 'wmt17-submitted-data/txt/system-outputs/newstest2017/{0}/newstest2017.{1}.{0}'.format(lang, system)) \n",
    "        DAF[lang] =  os.path.join(DATAHOME, 'DAseg-wmt-newstest2017/anon-proc-hits-seg-{}/analysis/ad-seg-scores.csv'.format(langREF))\n",
    "        \n",
    "        src, ref, hyp, da, sid = load17(srcF[lang], refF[lang], hypF[lang], DAF[lang], lang=lang)\n",
    "        data[lang] = {}\n",
    "        data[lang]['SRC'] = src\n",
    "        data[lang]['REF'] = ref\n",
    "        data[lang]['HYP'] = hyp\n",
    "        data[lang]['DA'] = da\n",
    "        data[lang]['SID'] = sid\n",
    "        \n",
    "# #         writeout('17', lang, data[lang])\n",
    "#         print('wmt17 {} : {} sentences'.format(lang, str(len(data[lang]['DA']))))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATAHOME, 'data15.pkl'), mode='rb') as r:\n",
    "    data15 = pickle.load(r)\n",
    "with open(os.path.join(DATAHOME, 'data16.pkl'), mode='rb') as r:\n",
    "    data16 = pickle.load(r)\n",
    "# with open(os.path.join(DATAHOME, 'data17.pkl'), mode='rb') as r:\n",
    "#     data17 = pickle.load(r)\n",
    "data17 = make17()\n",
    "with open(os.path.join(DATAHOME, 'data17.pkl'), mode='wb') as w:\n",
    "    pickle.dump(data17, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs-en : 560\n",
      "de-en : 560\n",
      "fi-en : 560\n",
      "lv-en : 560\n",
      "ru-en : 560\n",
      "tr-en : 560\n",
      "zh-en : 560\n"
     ]
    }
   ],
   "source": [
    "D = data17\n",
    "for lang in D.keys():\n",
    "    if not (len(D[lang]['REF']) == len(D[lang]['SRC']) == len(D[lang]['HYP']) == len(D[lang]['DA'])):\n",
    "        print('{} number of sentences does not match'.format(lang))\n",
    "    else:\n",
    "        print('{} : {}'.format(lang, len(D[lang]['REF'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# REFs = []\n",
    "# DUPs = []\n",
    "# for lang in data15.keys():\n",
    "#     for i in range(len(data15[lang]['REF'])):\n",
    "#         refsent = data15[lang]['REF'][i]\n",
    "#         if refsent not in REFs:\n",
    "#             REFs.append(refsent)\n",
    "#             data.append({'reference':refsent, \n",
    "#                          'source':data15[lang]['SRC'][i],\n",
    "#                          'language':lang, \n",
    "#                          'year':15, \n",
    "#                          'sent_num':i+1}\n",
    "#                         )\n",
    "#         else:\n",
    "#             DUPs.append({'reference':refsent, 'lang':lang, 'year':15, 'sid':i+1})\n",
    "# for lang in data16.keys():\n",
    "#     for i in range(len(data16[lang]['REF'])):\n",
    "#         refsent = data16[lang]['REF'][i]\n",
    "#         if refsent not in REFs:\n",
    "#             REFs.append(refsent)\n",
    "#             data.append({'reference':refsent, \n",
    "#                          'source':data16[lang]['SRC'][i],\n",
    "#                          'language':lang, \n",
    "#                          'year':16, \n",
    "#                          'sent_num':i+1}\n",
    "#                         )\n",
    "#         else:\n",
    "#             DUPs.append({'reference':refsent, 'lang':lang, 'year':16, 'sid':i+1})\n",
    "# for lang in data17.keys():\n",
    "#     for i in range(len(data17[lang]['REF'])):\n",
    "#         refsent = data17[lang]['REF'][i]\n",
    "#         if refsent not in REFs:\n",
    "#             REFs.append(refsent)\n",
    "#             data.append({'reference':refsent, \n",
    "#                          'source':data17[lang]['SRC'][i],\n",
    "#                          'language':lang, \n",
    "#                          'year':17, \n",
    "#                          'sent_num':data17[lang]['SID'][i]}\n",
    "#                         )\n",
    "#         else:\n",
    "#             DUPs.append({'reference':refsent, 'lang':lang, 'year':17, 'sid':i+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "REFs = []\n",
    "DUPs = []\n",
    "d_tmp = []\n",
    "for lang in data15.keys():\n",
    "    for i in range(len(data15[lang]['REF'])):\n",
    "        refsent = data15[lang]['REF'][i]\n",
    "#         if refsent not in REFs:\n",
    "#             REFs.append(refsent)\n",
    "        d_tmp.append({'language':str(lang), \n",
    "                     'year':int(15), \n",
    "                     'sent_num':int(i+1),\n",
    "                     'reference':str(refsent), \n",
    "                     'source':str(data15[lang]['SRC'][i]),\n",
    "                     'hypothesis':str(data15[lang]['HYP'][i]), \n",
    "                     'z-DAscore':float(data15[lang]['DA'][i])}\n",
    "                    )\n",
    "data = sorted(d_tmp, key=lambda k: k['source'])\n",
    "#         else:\n",
    "#             DUPs.append({'reference':refsent, 'lang':lang, 'year':15, 'sid':i+1})\n",
    "d_tmp = []\n",
    "for lang in data16.keys():\n",
    "    for i in range(len(data16[lang]['REF'])):\n",
    "        refsent = data16[lang]['REF'][i]\n",
    "#         if refsent not in REFs:\n",
    "#             REFs.append(refsent)\n",
    "        d_tmp.append({'language':str(lang), \n",
    "                     'year':int(16), \n",
    "                     'sent_num':int(i+1),\n",
    "                     'reference':str(refsent), \n",
    "                     'source':str(data16[lang]['SRC'][i]),\n",
    "                     'hypothesis':str(data16[lang]['HYP'][i]), \n",
    "                     'z-DAscore':float(data16[lang]['DA'][i])}\n",
    "                    )\n",
    "#         else:\n",
    "#             DUPs.append({'reference':refsent, 'lang':lang, 'year':16, 'sid':i+1})\n",
    "d_tmp = sorted(d_tmp, key=lambda k: k['source'])\n",
    "data.extend(d_tmp)\n",
    "d_tmp = []\n",
    "for lang in data17.keys():\n",
    "    for i in range(len(data17[lang]['REF'])):\n",
    "        refsent = data17[lang]['REF'][i]\n",
    "#         if refsent not in REFs:\n",
    "#             REFs.append(refsent)\n",
    "        d_tmp.append({'language':str(lang), \n",
    "                     'year':int(17), \n",
    "                     'sent_num':int(data17[lang]['SID'][i]),\n",
    "                     'reference':str(refsent), \n",
    "                     'source':str(data17[lang]['SRC'][i]),\n",
    "                     'hypothesis':str(data17[lang]['HYP'][i]), \n",
    "                     'z-DAscore':float(data17[lang]['DA'][i])}\n",
    "                   )\n",
    "#         else:\n",
    "#             DUPs.append({'reference':refsent, 'lang':lang, 'year':17, 'sid':i+1})\n",
    "d_tmp = sorted(d_tmp, key=lambda k: k['source'])\n",
    "data.extend(d_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATAHOME, 'wmt_data15-17_all_dup.pkl'), mode='wb') as w:\n",
    "    pickle.dump(data, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot create regular file '/ahc/ahcshare/Data/WMT/WMT_Metrics_Task/wmt_15_17_all_data.tsv': Permission denied\n"
     ]
    }
   ],
   "source": [
    "!cp $SAVEDIR /ahc/ahcshare/Data/WMT/WMT_Metrics_Task/wmt_15_17_all_data.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR = os.path.join('/project/nakamura-lab08/Work/kosuke-t/data/utils', 'wmt_15_17_all_data_dup.tsv')\n",
    "\n",
    "with open(SAVEDIR, \"w\", newline=\"\") as f:\n",
    "\n",
    "    # 「delimiter」に区切り文字、「quotechar」に囲い文字を指定します\n",
    "    # quotingにはクォーティング方針を指定します（後述）\n",
    "    writer = csv.writer(f, delimiter=\"\\t\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    # writerowに行列を指定することで1行分を出力できます\n",
    "    writer.writerow([\"year\", \"language\", \"sent_num\", \"source\", \"reference\", \"hypothesis\", \"z-DAscore\"])\n",
    "    keys = [\"year\", \"language\", \"sent_num\", \"source\", \"reference\", \"hypothesis\", \"z-DAscore\"]\n",
    "    for d in data:\n",
    "        writer.writerow([d[k] for k in keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'reference': \"It's particularly important if you're shooting with an action cam: These devices often have a fixed, wide angle lens, which means you have to get closer to your subjects if you want them to fill the picture frame.\",\n",
       "  'lang': 'cs-en',\n",
       "  'year': 15,\n",
       "  'sid': 72},\n",
       " {'reference': 'The squally conditions are believed to have contributed to the delayed landing of a Virgin Airlines flight from Melbourne to Adelaide.',\n",
       "  'lang': 'cs-en',\n",
       "  'year': 15,\n",
       "  'sid': 90},\n",
       " {'reference': 'That has spurred a massive civilian and military effort to fortify Mariupol, a steelmaking port of 500,000 that lies between Novoazovsk and the narrow gateway into the Crimean peninsula.',\n",
       "  'lang': 'cs-en',\n",
       "  'year': 15,\n",
       "  'sid': 117}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DUPs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REFs.index(DUPs[0]['reference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6998"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time apparently passes differently in the East than in the West.',\n",
       " \"It's particularly important if you're shooting with an action cam: These devices often have a fixed, wide angle lens, which means you have to get closer to your subjects if you want them to fill the picture frame.\",\n",
       " 'The main hearing will continue at the court in Hradec on Tuesday with testimony from court experts.',\n",
       " 'An inexcusable absence in your records will not have the same consequences as Section 53 (such as the inability to collect unemployment benefits), even though it hinders your ability to seek new employment.']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
