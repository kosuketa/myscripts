{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "import inspect\n",
    "import getpass\n",
    "import argparse\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "try:\n",
    "    from transformers import *\n",
    "except:\n",
    "    from pytorch_transformers import *\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.stats import pearsonr as pr\n",
    "from scipy.stats import spearmanr as sr\n",
    "import numpy as np\n",
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "import pickle\n",
    "import subprocess\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {'xlm':['xlm-mlm-en-2048',\n",
    "                 'xlm-mlm-ende-1024', \n",
    "                 'xlm-mlm-enfr-1024',\n",
    "                 'xlm-mlm-enro-1024',\n",
    "                 'xlm-mlm-xnli15-1024',\n",
    "                 'xlm-mlm-tlm-xnli15-1024',\n",
    "                 'xlm-clm-enfr-1024',\n",
    "                 'xlm-clm-ende-1024',\n",
    "                 'xlm-mlm-17-1280',\n",
    "                 'xlm-mlm-100-1280'], \n",
    "          'bert':['bert-base-uncased',\n",
    "                  'bert-large-uncased',\n",
    "                  'bert-base-cased',\n",
    "                  'bert-large-cased',\n",
    "                  'bert-base-multilingual-uncased',\n",
    "                  'bert-base-multilingual-cased',\n",
    "                  'bert-base-chinese',\n",
    "                  'bert-base-german-cased',\n",
    "                  'bert-large-uncased-whole-word-masking',\n",
    "                  'bert-large-cased-whole-word-masking',\n",
    "                  'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
    "                  'bert-large-cased-whole-word-masking-finetuned-squad',\n",
    "                  'bert-base-cased-finetuned-mrpc',\n",
    "                  'bert-base-german-dbmdz-cased',\n",
    "                  'bert-base-german-dbmdz-uncased',\n",
    "                  'cl-tohoku/bert-base-japanese',\n",
    "                  'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
    "                  'cl-tohoku/bert-base-japanese-char',\n",
    "                  'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
    "                  'TurkuNLP/bert-base-finnish-cased-v1',\n",
    "                  'TurkuNLP/bert-base-finnish-uncased-v1',\n",
    "                  'wietsedv/bert-base-dutch-cased'], \n",
    "          'xlm-r':['xlm-roberta-base', \n",
    "                   'xlm-roberta-large', \n",
    "                   \"xlm-roberta-large-finetuned-conll02-dutch\",\n",
    "                   \"xlm-roberta-large-finetuned-conll02-spanish\",\n",
    "                   \"xlm-roberta-large-finetuned-conll03-english\",\n",
    "                   \"xlm-roberta-large-finetuned-conll03-german\"],\n",
    "         'roberta':['roberta-base', \n",
    "                    'roberta-large', \n",
    "                    'roberta-large-mnli',\n",
    "                    'distilroberta-base',\n",
    "                    'roberta-base-openai-detector',\n",
    "                    'roberta-large-openai-detector'],\n",
    "         'reformer':['reformer-enwik8', \n",
    "                     'reformer-crime-and-punishment']\n",
    "#                      'google/reformer-enwik8',\n",
    "#                      'google/reformer-crime-and-punishment']\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_langs_id(modelname):\n",
    "    if modelname in MODELS['xlm']:\n",
    "        return True\n",
    "    \n",
    "    modelnames = []\n",
    "    for v in MODELS.values():\n",
    "        modelnames.extend(v)\n",
    "    assert modelname in modelnames\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_type(model_name):\n",
    "    if model_name in MODELS['bert']:\n",
    "        return 'bert'\n",
    "    elif model_name in MODELS['xlm']:\n",
    "        return 'xlm'\n",
    "    elif model_name in MODELS['xlm-r']:\n",
    "        return 'xlm-r'\n",
    "    elif model_name in MODELS['roberta']:\n",
    "        return 'roberta'\n",
    "    elif model_name in MODELS['reformer']:\n",
    "        return 'reformer'\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        exit(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_class(model_name):\n",
    "    if model_name in MODELS['bert']:\n",
    "        return BertTokenizer\n",
    "    elif model_name in MODELS['xlm']:\n",
    "        return XLMTokenizer\n",
    "    elif model_name in MODELS['xlm-r']:\n",
    "        return XLMRobertaTokenizer\n",
    "    elif model_name in MODELS['roberta']:\n",
    "        return RobertaTokenizer\n",
    "    elif model_name in MODELS['reformer']:\n",
    "        return ReformerTokenizer\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        exit(-2)\n",
    "        \n",
    "def get_model_class(model_name):\n",
    "    if model_name in MODELS['bert']:\n",
    "        return BertModel\n",
    "    elif model_name in MODELS['xlm']:\n",
    "        return XLMModel\n",
    "    elif model_name in MODELS['xlm-r']:\n",
    "        return XLMRobertaModel\n",
    "    elif model_name in MODELS['roberta']:\n",
    "        return RobertaModel\n",
    "    elif model_name in MODELS['reformer']:\n",
    "        return ReformerModel\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        exit(-2)\n",
    "        \n",
    "def get_config_class(model_name):\n",
    "    if model_name in MODELS['bert']:\n",
    "        return BertConfig\n",
    "    elif model_name in MODELS['xlm']:\n",
    "        return XLMConfig\n",
    "    elif model_name in MODELS['xlm-r']:\n",
    "        return XLMRobertaConfig\n",
    "    elif model_name in MODELS['roberta']:\n",
    "        return RobertaConfig\n",
    "    elif model_name in MODELS['reformer']:\n",
    "        return ReformerConfig\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        exit(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(parameters, s):\n",
    "    \"\"\"\n",
    "    Parse optimizer parameters.\n",
    "    Input should be of the form:\n",
    "        - \"sgd,lr=0.01\"\n",
    "        - \"adagrad,lr=0.1,lr_decay=0.05\"\n",
    "    \"\"\"\n",
    "    if \",\" in s:\n",
    "        method = s[:s.find(',')]\n",
    "        optim_params = {}\n",
    "        for x in s[s.find(',') + 1:].split(','):\n",
    "            split = x.split('=')\n",
    "            assert len(split) == 2\n",
    "            assert re.match(\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\", split[1]) is not None\n",
    "            optim_params[split[0]] = float(split[1])\n",
    "    else:\n",
    "        method = s\n",
    "        optim_params = {}\n",
    "\n",
    "    if method == 'adadelta':\n",
    "        optim_fn = optim.Adadelta\n",
    "    elif method == 'adagrad':\n",
    "        optim_fn = optim.Adagrad\n",
    "    elif method == 'adam':\n",
    "        optim_fn = optim.Adam\n",
    "        optim_params['betas'] = (optim_params.get('beta1', 0.9), optim_params.get('beta2', 0.999))\n",
    "        optim_params.pop('beta1', None)\n",
    "        optim_params.pop('beta2', None)\n",
    "    elif method == 'adam_inverse_sqrt':\n",
    "        optim_fn = AdamInverseSqrtWithWarmup\n",
    "        optim_params['betas'] = (optim_params.get('beta1', 0.9), optim_params.get('beta2', 0.999))\n",
    "        optim_params.pop('beta1', None)\n",
    "        optim_params.pop('beta2', None)\n",
    "    elif method == 'adamax':\n",
    "        optim_fn = optim.Adamax\n",
    "    elif method == 'asgd':\n",
    "        optim_fn = optim.ASGD\n",
    "    elif method == 'rmsprop':\n",
    "        optim_fn = optim.RMSprop\n",
    "    elif method == 'rprop':\n",
    "        optim_fn = optim.Rprop\n",
    "    elif method == 'sgd':\n",
    "        optim_fn = optim.SGD\n",
    "        assert 'lr' in optim_params\n",
    "    else:\n",
    "        raise Exception('Unknown optimization method: \"%s\"' % method)\n",
    "\n",
    "    # check that we give good parameters to the optimizer\n",
    "    expected_args = inspect.getargspec(optim_fn.__init__)[0]\n",
    "    assert expected_args[:2] == ['self', 'params']\n",
    "    if not all(k in expected_args[2:] for k in optim_params.keys()):\n",
    "        raise Exception('Unexpected parameters: expected \"%s\", got \"%s\"' % (\n",
    "            str(expected_args[2:]), str(optim_params.keys())))\n",
    "\n",
    "    return optim_fn(parameters, **optim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pearson(pred, true):\n",
    "    try:\n",
    "        r, p_value = pr(np.asarray(pred), np.asarray(true))\n",
    "    except ValueError:\n",
    "        r = -1.0\n",
    "    return r\n",
    "\n",
    "def calc_spearman(pred, true):\n",
    "    try:\n",
    "        r, p_value = sr(np.asarray(pred), np.asarray(true))\n",
    "    except ValueError:\n",
    "        r = -1.0\n",
    "    return r\n",
    "def mse(pred, true):\n",
    "    return (np.square(pred - true)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_gpu(*args):\n",
    "    for arg in args:\n",
    "        if arg != None:\n",
    "            del arg\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(args):\n",
    "    ModelClass = get_model_class(args.model_name)\n",
    "    ConfigClass = get_config_class(args.model_name)\n",
    "    \n",
    "    config = ConfigClass.from_pretrained(args.model_name)\n",
    "    model = ModelClass.from_pretrained(args.model_name, config=config)\n",
    "    model.config.num_labels = 1\n",
    "    if args.hyp_src_hyp_ref:\n",
    "        model.mlp = nn.Sequential(*[nn.Dropout(args.dropout),nn.Linear(model.config.hidden_size*2, 1)])\n",
    "    else:\n",
    "        model.mlp = nn.Sequential(*[nn.Dropout(args.dropout),nn.Linear(model.config.hidden_size, 1)])\n",
    "    \n",
    "    optimizer = get_optimizer(list(model.parameters()), args.optimizer)\n",
    "    mse = nn.MSELoss()\n",
    "    \n",
    "    return model, config, optimizer, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(results, exp_name, dump_path):\n",
    "    results = results['test']\n",
    "    \n",
    "    X = results['true']\n",
    "    Y = results['pred']\n",
    "    figdir1 = os.path.join(dump_path, '{}_scatter.png'.format(exp_name))\n",
    "    figdir2 = os.path.join(dump_path, '{}_scatter.pdf'.format(exp_name))\n",
    "    fig = plt.figure(figsize=(7, 5), dpi=100)\n",
    "    plt.scatter(X, Y)\n",
    "    plt.xlabel('DA score')\n",
    "    plt.ylabel('prediction')\n",
    "    plt.grid()\n",
    "    plt.savefig(figdir1)\n",
    "    pp = PdfPages(figdir2)\n",
    "    pp.savefig(fig)\n",
    "    pp.close()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_meta_evaluation(results):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Performance of This Model (Pearson)---\n",
      "all : 0.611\n"
     ]
    }
   ],
   "source": [
    "# dump_path = '/ahc/work3/kosuke-t/SRHDA/transformers/log/'\n",
    "# exp_name = 'multiBERT_all_hyp_src'\n",
    "# dump_path = os.path.join(os.path.join(dump_path, exp_name), '1')\n",
    "# with open(os.path.join(dump_path, 'result.pkl'), mode='rb') as f:\n",
    "#     results = pickle.load(f)\n",
    "# plot_scatter(results, exp_name, dump_path)\n",
    "\n",
    "# best_valid_pearson = -1.0\n",
    "# best_valid_epoch = 0\n",
    "# for e, v_pearson in enumerate(results['valid']['pearson']):\n",
    "#     if best_valid_pearson < v_pearson:\n",
    "#         best_valid_pearson = v_pearson\n",
    "#         best_valid_epoch = e\n",
    "# print('--- Final Performance of This Model (Pearson)---')\n",
    "# print('all : {:.3f}'.format(results['test']['pearson'][best_valid_epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "[10]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
