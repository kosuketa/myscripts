{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (dataloader_debug.py, line 161)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/is/kosuke-t/.pyenv/versions/anaconda3-5.3.1/envs/torch-tensor/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3296\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-6ce17fdfa751>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from dataloader_debug import Dataset\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/is/kosuke-t/bert-related/utils/torch-bert/xlm_r/debug/dataloader_debug.py\"\u001b[0;36m, line \u001b[0;32m161\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.stats import pearsonr as pr\n",
    "from scipy.stats import spearmanr as sr\n",
    "import copy\n",
    "import pandas as pd\n",
    "import difflib\n",
    "from transformers import *\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from apex import amp\n",
    "from torch import optim\n",
    "from typing import Tuple\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn\n",
    "import utils\n",
    "from shutil import rmtree\n",
    "import apex\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import logging\n",
    "random.seed(77)\n",
    "torch.manual_seed(77)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from dataloader_debug import Dataset, Data_Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bool_flag(s):\n",
    "    \"\"\"\n",
    "    Parse boolean arguments from the command line.\n",
    "    \"\"\"\n",
    "    FALSY_STRINGS = {'off', 'false', '0'}\n",
    "    TRUTHY_STRINGS = {'on', 'true', '1'}\n",
    "    if s.lower() in FALSY_STRINGS:\n",
    "        return False\n",
    "    elif s.lower() in TRUTHY_STRINGS:\n",
    "        return True\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(\"Invalid value for a boolean flag!\")\n",
    "\n",
    "# return True when only 1 item is True and others are all False\n",
    "def bool_check_onlyone_stands(bools):\n",
    "    length = len(bools)\n",
    "    length_ls = list(range(length))\n",
    "    for i in range(length):\n",
    "        if bools[i]:\n",
    "            copy_length_ls = copy.deepcopy(length_ls)\n",
    "            copy_length_ls.pop(i)\n",
    "            ex_is = copy_length_ls\n",
    "            for e_i in ex_is:\n",
    "                if bools[e_i]:\n",
    "                    return False\n",
    "            return True\n",
    "        else:\n",
    "            continue\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# general setting\n",
    "parser.add_argument('--exp_name', type=str, default='test')\n",
    "parser.add_argument('--exp_id', type=str, default='1')\n",
    "parser.add_argument('--dump_path', type=str, default='/ahc/work3/kosuke-t/SRHDA/XLM/log/')\n",
    "parser.add_argument('--model_name', type=str, default='bert-base-uncased')\n",
    "parser.add_argument('--empty_dump', type=bool_flag, default=False)\n",
    "parser.add_argument('--train', type=bool_flag, default=True)\n",
    "parser.add_argument('--test', type=bool_flag, default=True)\n",
    "\n",
    "# hyperparameters\n",
    "parser.add_argument('--batch_size', type=int, default=8)\n",
    "parser.add_argument('--epoch_size', type=int, default=3)\n",
    "parser.add_argument('--optimizer', type=str, default='adam,lr=0.00001')\n",
    "parser.add_argument('--lr_lambda', type=float, default=0.707, help='lr decay rate')\n",
    "parser.add_argument('--dropout', type=float, default=0.0, help='mlp dropout')\n",
    "\n",
    "# model setting\n",
    "parser.add_argument('--amp', type=bool_flag, default=False)\n",
    "parser.add_argument('--load_model', type=bool_flag, default=False)\n",
    "parser.add_argument('--load_model_path', type=str, default='')\n",
    "parser.add_argument('--save_model_name', type=str, default='model.pth')\n",
    "parser.add_argument('--save_model_path', type=str, default='')\n",
    "\n",
    "# data setting\n",
    "parser.add_argument('--src_train', type=str, default='', help='src data path for train')\n",
    "parser.add_argument('--src_valid', type=str, default='', help='src data path for train')\n",
    "parser.add_argument('--src_test', type=str, default='', help='src data path for train')\n",
    "parser.add_argument('--label_train', type=str, default='', help='label data path for train')\n",
    "parser.add_argument('--label_valid', type=str, default='', help='label data path for train')\n",
    "parser.add_argument('--label_test', type=str, default='', help='label data path for train')\n",
    "\n",
    "parser.add_argument('--train_shrink', type=float, default=1.0)\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# make dump_path\n",
    "args.dump_path = os.path.join(os.path.join(args.dump_path, args.exp_name), args.exp_id)\n",
    "if not os.path.isdir(args.dump_path):\n",
    "    os.makedirs(args.dump_path)\n",
    "elif args.empty_dump:\n",
    "    rmtree(args.dump_path)\n",
    "    os.makedirs(args.dump_path)\n",
    "if args.save_model_path == '':\n",
    "    args.save_model_path = os.path.join(args.dump_path, args.save_model_name)\n",
    "\n",
    "args.data_paths_train = [args.src_train, args.label_train]\n",
    "args.data_paths_valid = [args.src_valid, args.label_valid]\n",
    "args.data_paths_test = [args.src_test, args.label_test]\n",
    "        \n",
    "if not (args.train or args.test):\n",
    "    print('ERROR: argument -train or -test, either of them must be true!')\n",
    "    exit(-2)\n",
    "    \n",
    "logging.basicConfig(filename=os.path.join(args.dump_path, 'logger.log'), level=logging.INFO)\n",
    "args.logger = logging\n",
    "data_paths = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "data_trans = Data_Transformer(args, tokenizer)\n",
    "DATA = {}\n",
    "if args.train:\n",
    "    DATA['train'] = Dataset(data_trans, tokenizer, args.data_paths_train, args, '{}.train'.format(args.exp_name))\n",
    "    DATA['valid'] = Dataset(data_trans, tokenizer, args.data_paths_valid, args, '{}.valid'.format(args.exp_name))\n",
    "if args.test:\n",
    "    DATA['test'] = Dataset(data_trans, tokenizer, args.data_paths_test, args, '{}.test'.format(args.exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(optimizer, args):\n",
    "    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * args.lr_lambda\n",
    "    return optimizer\n",
    "\n",
    "def run_model(model, batch, args, loss_fn, optimizer, train=False):\n",
    "    x = batch['src'].to('cuda')\n",
    "    seg_x = batch['seg_src'].to('cuda')\n",
    "    labels = batch['label'].to('cuda')\n",
    "    \n",
    "    h = model(x, token_type_ids=seg_x)[1]\n",
    "    preds = model.mlp(h)\n",
    "    loss = loss_fn(preds.view(-1), labels.view(-1))\n",
    "    \n",
    "    preds = [float(p) for p in preds.view(-1).cpu().detach().numpy()]\n",
    "    labels = [float(t) for t in labels.view(-1).cpu().detach().numpy()]\n",
    "    \n",
    "    if train:\n",
    "        if not args.amp:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with apex.amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return loss.item(), preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pdb;pdb.set_trace()\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(DATA['train'], \n",
    "                                               batch_size=args.batch_size, \n",
    "                                               collate_fn=data_trans.collate_fn, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(DATA['valid'], batch_size=args.batch_size,\n",
    "                                               collate_fn=data_trans.collate_fn, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(DATA['test'], batch_size=args.batch_size,\n",
    "                                              collate_fn=data_trans.collate_fn, shuffle=False)\n",
    "\n",
    "model = AutoModel.from_pretrained(args.model_name)\n",
    "model.config.num_labels = 1\n",
    "model.mlp = nn.Sequential(*[nn.Dropout(args.dropout),nn.Linear(model.config.hidden_size, 1)]).cuda()\n",
    "model.to('cuda')\n",
    "optimizer = utils.get_optimizer(list(model.parameters()), args.optimizer)\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "if args.amp:\n",
    "    model, optimizer = apex.amp.initialize(\n",
    "        model,\n",
    "        optimizer,\n",
    "        opt_level=('O%i' % 1)\n",
    "    )\n",
    "\n",
    "results = {mode:{key:[] for key in ['loss', 'pred', 'true']} for mode in ['train', 'valid', 'test']}\n",
    "best_val_loss = 10000\n",
    "for n_epoch in range(args.epoch_size):\n",
    "    # train\n",
    "    if args.train:\n",
    "        model.train()\n",
    "        losses = []\n",
    "        preds_ls = []\n",
    "        trues_ls = []\n",
    "        for batch_data in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss, preds, labels = run_model(model, batch_data, args, mse, optimizer, train=True)\n",
    "            losses.append(loss)\n",
    "            preds_ls.extend(preds)\n",
    "            trues_ls.extend(labels)\n",
    "        \n",
    "        results['train']['loss'].append(np.mean(losses))\n",
    "        results['train']['pred'].append(preds_ls)\n",
    "        results['train']['true'].append(trues_ls)\n",
    "            \n",
    "        # valid\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        preds_ls = []\n",
    "        trues_ls = []\n",
    "        for batch_data in valid_dataloader:\n",
    "            with torch.no_grad():\n",
    "                loss, preds, labels = run_model(model, batch_data, args, mse, optimizer)\n",
    "            losses.append(loss)\n",
    "            preds_ls.extend(preds)\n",
    "            trues_ls.extend(labels)\n",
    "            \n",
    "        results['valid']['loss'].append(np.mean(losses))\n",
    "        results['valid']['pred'].append(preds_ls)\n",
    "        results['valid']['true'].append(trues_ls)\n",
    "        \n",
    "        # update lr\n",
    "        if best_val_loss > np.mean(losses):\n",
    "            best_val_loss = np.mean(losses)\n",
    "        else:\n",
    "            optimizer = update_lr(optimizer, args)\n",
    "        \n",
    "    # test\n",
    "    if args.test:\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        preds_ls = []\n",
    "        trues_ls = []\n",
    "        for batch_data in test_dataloader:\n",
    "            with torch.no_grad():\n",
    "                loss, preds, labels = run_model(model, batch_data, args, mse, optimizer)\n",
    "            losses.append(loss)\n",
    "            preds_ls.extend(preds)\n",
    "            trues_ls.extend(labels)\n",
    "            \n",
    "        results['test']['loss'].append(np.mean(losses))\n",
    "        results['test']['pred'].append(preds_ls)\n",
    "        results['test']['true'].append(trues_ls)   \n",
    "    \n",
    "    print('-----------------')\n",
    "    print('{}epoch finished!'.format(n_epoch+1))\n",
    "    print('lr = {}'.format(optimizer.param_groups[0]['lr']))\n",
    "    if args.train:\n",
    "        print('train loss_mean:{:.4f}'.format(results['train']['loss'][-1]))\n",
    "        print('valid loss_mean:{:.4f}'.format(results['valid']['loss'][-1]))\n",
    "    if args.test:\n",
    "        print('test loss_mean:{:.4f}'.format(results['test']['loss'][-1]))\n",
    "        \n",
    "    print('-----------------')\n",
    "    \n",
    "with open(os.path.join(args.dump_path, 'result.pkl'), mode='wb') as w:\n",
    "    pickle.dump(results, w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bash run.sh --exp_name multiBERT_all_hyp_ref --optimizer adam,lr=0.00003 --batch_size 16 --epoch_size 20 --hyp_src True --hyp_src_hyp_ref False\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
