{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.stats import pearsonr as pr\n",
    "from scipy.stats import spearmanr as sr\n",
    "import copy\n",
    "import pandas as pd\n",
    "import difflib\n",
    "from transformers import *\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from apex import amp\n",
    "from torch import optim\n",
    "from typing import Tuple\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn\n",
    "\n",
    "from shutil import rmtree\n",
    "\n",
    "import logging\n",
    "random.seed(77)\n",
    "torch.manual_seed(77)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset \n",
    "must be build independently from each mode such as 'train', 'dev', 'test:de', test:cs'\n",
    "\n",
    "preprocessing:\n",
    "    read data from data_paths\n",
    "    tokenize data\n",
    "    make lang_token\n",
    "    uniform length of mini-batch data and insert pad token\n",
    "    \n",
    "\"\"\"\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform, tokenizer, data_paths, params, data_name=None):\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_paths = data_paths\n",
    "        self.args = params\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        self.savedata_dir = os.path.join(params.dump_path, '{}.pkl'.format(data_name))\n",
    "        if not os.path.isfile(self.savedata_dir):\n",
    "            self.data = self.read_data(self.data_paths, tokenizer)\n",
    "            with open(self.savedata_dir, mode='wb') as w:\n",
    "                pickle.dump(self.data, w)\n",
    "        else:\n",
    "            with open(self.savedata_dir, mode='rb') as r:\n",
    "                self.data = pickle.load(r)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            out_data = self.transform(out_data)\n",
    "\n",
    "        return out_data\n",
    "    \n",
    "    def get_seqment_id(self, tokens):\n",
    "        x = []\n",
    "        x += [0] * len(tokens)\n",
    "        return x\n",
    "        \n",
    "    ### add tokenizing process\n",
    "    def read_data(self, data_paths, tokenizer):\n",
    "        forms = ['src', 'label']\n",
    "        DATA = {form:None for form in forms}\n",
    "        if tokenizer.bos_token_id != None:\n",
    "            bos_id = tokenizer.bos_token_id\n",
    "        else:\n",
    "            bos_id = tokenizer.cls_token_id\n",
    "        if tokenizer.eos_token_id != None:\n",
    "            eos_id = tokenizer.eos_token_id\n",
    "        else:\n",
    "            eos_id = tokenizer.sep_token_id\n",
    "        if tokenizer.sep_token_id != None:\n",
    "            sep_id = tokenizer.sep_token_id\n",
    "        else:\n",
    "            sep_id = tokenizer.eos_token_id\n",
    "        \n",
    "        for data_path, form in zip(data_paths, forms):\n",
    "            assert os.path.isfile(data_path)\n",
    "            with open(data_path, mode='r', encoding='utf-8') as r:\n",
    "                data = r.read().split(os.linesep)\n",
    "                if data[-1] == '':\n",
    "                    data.pop(-1)\n",
    "            DATA[form] = data\n",
    "        r_data = []\n",
    "        for i in range(len(DATA[forms[0]])):\n",
    "            tmp_dic = {}\n",
    "            for form in forms:\n",
    "                d = DATA[form][i]\n",
    "                if form == 'label':\n",
    "                    tmp_dic['{}'.format(form)] = float(d)\n",
    "                else:\n",
    "                    tmp_dic['raw_{}'.format(form)] = d\n",
    "                    \n",
    "            tmp_dic['tok_src'] = self.tokenizer.encode(tmp_dic['raw_src'])\n",
    "            tmp_dic['seg_src'] = self.get_seqment_id(tmp_dic['tok_src'])\n",
    "            r_data.append(tmp_dic)\n",
    "        \n",
    "        return r_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Transformer():\n",
    "    def __init__(self, args, tokenizer):\n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "#         import pdb;pdb.set_trace()\n",
    "        return batch\n",
    "\n",
    "    def padding(self, tok_list, pad_id=None, lang_padding=False):\n",
    "        max_seq_len = max([len(x) for x in tok_list])\n",
    "        bs_size = len(tok_list)\n",
    "        new_tok_list = []\n",
    "        for toks in tok_list:\n",
    "            if pad_id == None:\n",
    "                if lang_padding:\n",
    "                    toks += [toks[-1]]*(max_seq_len-len(toks))\n",
    "                else:\n",
    "                    toks += [self.pad_id]*(max_seq_len-len(toks))   \n",
    "            else:\n",
    "                toks += [pad_id]*(max_seq_len-len(toks))\n",
    "            new_tok_list.append(toks)\n",
    "        x = torch.tensor(new_tok_list)\n",
    "        return x\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        tok_src = []\n",
    "        seg_src = []\n",
    "        return_dic = {'raw_src':[], \n",
    "                      'label':[]\n",
    "                     }\n",
    "        for btch in batch:\n",
    "            return_dic['raw_src'].append(btch['raw_src'])\n",
    "            return_dic['label'].append(float(btch['label']))\n",
    "            tok_src.append(btch['tok_src'])\n",
    "            seg_src.append(btch['seg_src'])\n",
    "\n",
    "        return_dic['src'] = self.padding(tok_src)\n",
    "        return_dic['seg_src'] = self.padding(seg_src, pad_id=0)\n",
    "        \n",
    "        return_dic['label'] = torch.FloatTensor(return_dic['label'])\n",
    "    \n",
    "        return return_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# a = torch.ones(1, 5)\n",
    "# b = torch.ones(1, 8)\n",
    "# print(a)\n",
    "# print(b)\n",
    "\n",
    "# torch.nn.utils.rnn.pad_sequence([b, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
