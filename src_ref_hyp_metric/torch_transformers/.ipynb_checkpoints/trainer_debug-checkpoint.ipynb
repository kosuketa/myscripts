{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (dataloader_debug.py, line 161)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/is/kosuke-t/.pyenv/versions/anaconda3-5.3.1/envs/torch-tensor/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3296\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-6ce17fdfa751>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from dataloader_debug import Dataset\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/is/kosuke-t/bert-related/utils/torch-bert/xlm_r/debug/dataloader_debug.py\"\u001b[0;36m, line \u001b[0;32m161\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.stats import pearsonr as pr\n",
    "from scipy.stats import spearmanr as sr\n",
    "import copy\n",
    "import pandas as pd\n",
    "import difflib\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from apex import amp\n",
    "from torch import optim\n",
    "from typing import Tuple\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn\n",
    "import utils\n",
    "from shutil import rmtree\n",
    "import apex\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "import logging\n",
    "random.seed(77)\n",
    "# torch.manual_seed(77)\n",
    "# np.random.seed(0)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from dataloader_debug import Dataset, Data_Transformer\n",
    "from pytorch_memlab import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bool_flag(s):\n",
    "    \"\"\"\n",
    "    Parse boolean arguments from the command line.\n",
    "    \"\"\"\n",
    "    FALSY_STRINGS = {'off', 'false', '0'}\n",
    "    TRUTHY_STRINGS = {'on', 'true', '1'}\n",
    "    if s.lower() in FALSY_STRINGS:\n",
    "        return False\n",
    "    elif s.lower() in TRUTHY_STRINGS:\n",
    "        return True\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(\"Invalid value for a boolean flag!\")\n",
    "\n",
    "# return True when only 1 item is True and others are all False\n",
    "def bool_check_onlyone_stands(bools):\n",
    "    length = len(bools)\n",
    "    length_ls = list(range(length))\n",
    "    for i in range(length):\n",
    "        if bools[i]:\n",
    "            copy_length_ls = copy.deepcopy(length_ls)\n",
    "            copy_length_ls.pop(i)\n",
    "            ex_is = copy_length_ls\n",
    "            for e_i in ex_is:\n",
    "                if bools[e_i]:\n",
    "                    return False\n",
    "            return True\n",
    "        else:\n",
    "            continue\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# general setting\n",
    "parser.add_argument('--exp_name', type=str, default='test')\n",
    "parser.add_argument('--exp_id', type=str, default='1')\n",
    "parser.add_argument('--trial_times', type=int, default=10)\n",
    "parser.add_argument('--n_trial', type=int, default=1)\n",
    "parser.add_argument('--dump_path', type=str, default='/ahc/work3/kosuke-t/SRHDA/XLM/log/')\n",
    "parser.add_argument('--model_name', type=str, default='bert-base-uncased')\n",
    "parser.add_argument('--langs', type=str, default='cs-en,de-en,fi-en,lv-en,ro-en,ru-en,tr-en,zh-en')\n",
    "parser.add_argument('--empty_dump', type=bool_flag, default=False)\n",
    "parser.add_argument('--train', type=bool_flag, default=True)\n",
    "parser.add_argument('--test', type=bool_flag, default=True)\n",
    "\n",
    "# hyperparameters\n",
    "parser.add_argument('--batch_size', type=str, default='batch=8')\n",
    "parser.add_argument('--epoch_size', type=int, default=3)\n",
    "parser.add_argument('--optimizer', type=str, default='adam,lr=0.00001')\n",
    "parser.add_argument('--lr_lambda', type=float, default=0.707, help='lr decay rate')\n",
    "parser.add_argument('--dropout', type=float, default=0.0, help='mlp dropout')\n",
    "\n",
    "# model setting\n",
    "parser.add_argument('--amp', type=bool_flag, default=False)\n",
    "parser.add_argument('--load_model', type=bool_flag, default=False)\n",
    "parser.add_argument('--load_model_path', type=str, default='')\n",
    "parser.add_argument('--save_model_name', type=str, default='model.pth')\n",
    "parser.add_argument('--save_model_path', type=str, default='')\n",
    "parser.add_argument('--hyp_ref', type=bool_flag, default=False)\n",
    "parser.add_argument('--hyp_src', type=bool_flag, default=False)\n",
    "parser.add_argument('--hyp_src_hyp_ref', type=bool_flag, default=False)\n",
    "parser.add_argument('--hyp_src_ref', type=bool_flag, default=False)\n",
    "\n",
    "# data setting\n",
    "parser.add_argument('--src_train', type=str, default='', help='src data path for train')\n",
    "parser.add_argument('--src_valid', type=str, default='', help='src data path for train')\n",
    "parser.add_argument('--src_test', type=str, default='', help='src data path for train')\n",
    "parser.add_argument('--ref_train', type=str, default='', help='ref data path for train')\n",
    "parser.add_argument('--ref_valid', type=str, default='', help='ref data path for train')\n",
    "parser.add_argument('--ref_test', type=str, default='', help='ref data path for train')\n",
    "parser.add_argument('--hyp_train', type=str, default='', help='hyp data path for train')\n",
    "parser.add_argument('--hyp_valid', type=str, default='', help='hyp data path for train')\n",
    "parser.add_argument('--hyp_test', type=str, default='', help='hyp data path for train')\n",
    "parser.add_argument('--label_train', type=str, default='', help='label data path for train')\n",
    "parser.add_argument('--label_valid', type=str, default='', help='label data path for train')\n",
    "parser.add_argument('--label_test', type=str, default='', help='label data path for train')\n",
    "\n",
    "parser.add_argument('--train_shrink', type=float, default=1.0)\n",
    "parser.add_argument('--debug', type=bool_flag, default=False)\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# make dump_path\n",
    "args.dump_path = os.path.join(os.path.join(args.dump_path, args.exp_name), args.exp_id)\n",
    "if not os.path.isdir(args.dump_path):\n",
    "    os.makedirs(args.dump_path)\n",
    "elif args.empty_dump:\n",
    "    rmtree(args.dump_path)\n",
    "    os.makedirs(args.dump_path)\n",
    "if args.save_model_path == '':\n",
    "    args.save_model_path = os.path.join(args.dump_path, args.save_model_name)\n",
    "    \n",
    "args.langs = args.langs.split(',')\n",
    "\n",
    "# bool_flags check\n",
    "assert bool_check_onlyone_stands([args.hyp_ref, args.hyp_src, args.hyp_src_hyp_ref, args.hyp_src_ref])\n",
    "\n",
    "# check if lang_ids is needed as inputs to the model\n",
    "args.lang_id_bool = utils.check_for_langs_id(args.model_name)\n",
    "\n",
    "src_train_flag = os.path.isfile(args.src_train) and os.path.isfile(args.src_valid)\n",
    "src_test_flag = os.path.isfile(args.src_test)\n",
    "ref_train_flag = os.path.isfile(args.ref_train) and os.path.isfile(args.ref_valid)\n",
    "ref_test_flag = os.path.isfile(args.ref_test)\n",
    "hyp_train_flag = os.path.isfile(args.hyp_train) and os.path.isfile(args.hyp_valid)\n",
    "hyp_test_flag = os.path.isfile(args.hyp_test)\n",
    "label_train_flag = os.path.isfile(args.label_train) and os.path.isfile(args.label_valid)\n",
    "label_test_flag = os.path.isfile(args.label_test)\n",
    "if args.train:\n",
    "    if args.hyp_ref:\n",
    "        assert hyp_train_flag and ref_train_flag\n",
    "        args.forms = ['ref', 'hyp', 'label']\n",
    "#         args.data_paths_train = [args.ref_train, args.hyp_train, args.label_train]\n",
    "#         args.data_paths_valid = [args.ref_valid, args.hyp_valid, args.label_valid]\n",
    "    elif args.hyp_src:\n",
    "        assert hyp_train_flag and src_train_flag\n",
    "        args.forms = ['src', 'hyp', 'label']\n",
    "#         args.data_paths_train = [args.src_train, args.hyp_train, args.label_train]\n",
    "#         args.data_paths_valid = [args.src_valid, args.hyp_valid, args.label_valid]\n",
    "    elif args.hyp_src_hyp_ref or args.hyp_src_ref:\n",
    "        assert hyp_train_flag and src_train_flag and ref_train_flag\n",
    "        args.forms = ['src', 'ref', 'hyp', 'label']\n",
    "    args.data_paths_train = [args.src_train, args.ref_train, args.hyp_train, args.label_train]\n",
    "    args.data_paths_valid = [args.src_valid, args.ref_valid, args.hyp_valid, args.label_valid]\n",
    "if args.test:\n",
    "    if args.hyp_ref:\n",
    "        assert hyp_test_flag and ref_test_flag\n",
    "        args.forms = ['ref', 'hyp', 'label']\n",
    "#         args.data_paths_test = [args.ref_test, args.hyp_test, args.label_test]\n",
    "    elif args.hyp_src:\n",
    "        assert hyp_test_flag and src_test_flag\n",
    "        args.forms = ['src', 'hyp', 'label']\n",
    "#         args.data_paths_test = [args.src_test, args.hyp_test, args.label_test]\n",
    "    elif args.hyp_src_hyp_ref or args.hyp_src_ref:\n",
    "        assert hyp_test_flag and src_test_flag and ref_test_flag\n",
    "        args.forms = ['src', 'ref', 'hyp', 'label']\n",
    "    args.data_paths_test = [args.src_test, args.ref_test, args.hyp_test, args.label_test]\n",
    "        \n",
    "    if not args.train:\n",
    "        args.load_model = True\n",
    "        if os.path.isfile(args.load_model_path):\n",
    "            print('ERRORL: model to be loaded does not exist!')\n",
    "            exit(-2)\n",
    "if not (args.train or args.test):\n",
    "    print('ERROR: argument -train or -test, either of th”’em must be true!')\n",
    "    exit(-2)\n",
    "\n",
    "if utils.get_model_type in ['bert']:\n",
    "    args.use_token_type_ids = True\n",
    "else:\n",
    "    args.use_token_type_ids = False\n",
    "    \n",
    "logging.basicConfig(filename=os.path.join(args.dump_path, 'logger.log'), level=logging.INFO)\n",
    "args.logger = logging\n",
    "\n",
    "args.batch_size = int(args.batch_size.split('=')[-1])\n",
    "\n",
    "txt = \"\"\n",
    "for key, value in args.__dict__.items():\n",
    "    txt += '{}:{}{}'.format(str(key), str(value), os.linesep)\n",
    "with open(os.path.join(args.dump_path, 'arguments.txt'), mode='w', encoding='utf-8') as w:\n",
    "    w.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(optimizer, args):\n",
    "    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * args.lr_lambda\n",
    "    return optimizer\n",
    "\n",
    "# @profile\n",
    "def run_model(model, batch, args, loss_fn, optimizer, train=False):\n",
    "    if args.hyp_src_hyp_ref:\n",
    "        x1 = batch['hyp_src'].to('cuda')\n",
    "        if args.lang_id_bool:\n",
    "            seg_x1 = batch['lang_hyp_src'].to('cuda')\n",
    "        elif args.use_token_type_ids:\n",
    "            seg_x1 = batch['seg_hyp_src'].to('cuda')\n",
    "        x2 = batch['hyp_ref'].to('cuda')\n",
    "        if args.lang_id_bool:\n",
    "            seg_x2 = batch['lang_hyp_ref'].to('cuda')\n",
    "        elif args.use_token_type_ids:\n",
    "            seg_x2 = batch['seg_hyp_ref'].to('cuda')\n",
    "        \n",
    "        if args.lang_id_bool:\n",
    "            h1 = model(x1, langs=seg_x1)[0][:,0,:]\n",
    "            h2 = model(x2, langs=seg_x2)[0][:,0,:]\n",
    "            \n",
    "        elif args.use_token_type_ids:\n",
    "            out1 = model(x1, token_type_ids=seg_x1)\n",
    "            h1 = model(x1, token_type_ids=seg_x1)[1]\n",
    "            h2 = model(x2, token_type_ids=seg_x2)[1]\n",
    "        else:\n",
    "            out1 = model(x1)\n",
    "            h1 = model(x1)[1]\n",
    "            h2 = model(x2)[1]\n",
    "        h = torch.cat([h1,h2], dim=1)\n",
    "    \n",
    "    elif args.hyp_src:\n",
    "        x1 = batch['hyp_src'].to('cuda')\n",
    "        if args.lang_id_bool:\n",
    "            seg_x1 = batch['lang_hyp_src'].to('cuda')\n",
    "        elif args.use_token_type_ids:\n",
    "            seg_x1 = batch['seg_hyp_src'].to('cuda')\n",
    "        \n",
    "        if args.lang_id_bool:\n",
    "            h = model(x1, langs=seg_x1)[0]\n",
    "        elif args.use_token_type_ids:\n",
    "            h = model(x1, token_type_ids=seg_x1)[1]\n",
    "        else:\n",
    "            h = model(x1)[1]\n",
    "        \n",
    "    elif args.hyp_ref:\n",
    "        x2 = batch['hyp_ref'].to('cuda')\n",
    "        if args.lang_id_bool:\n",
    "            seg_x2 = batch['lang_hyp_ref'].to('cuda')\n",
    "        elif args.use_token_type_ids:\n",
    "            seg_x2 = batch['seg_hyp_ref'].to('cuda')\n",
    "        \n",
    "        if args.lang_id_bool:\n",
    "            h = model(x2, langs=seg_x2)[0]\n",
    "        elif args.use_token_type_ids:\n",
    "            h = model(x2, token_type_ids=seg_x2)[1]\n",
    "        else:\n",
    "            h = model(x2)[1]\n",
    "            \n",
    "    elif args.hyp_src_ref:\n",
    "        x1 = batch['hyp_src_ref'].to('cuda')\n",
    "        if args.lang_id_bool:\n",
    "            seg_x1 = batch['lang_hyp_src_ref'].to('cuda')\n",
    "        elif args.use_token_type_ids:\n",
    "            seg_x1 = batch['seg_hyp_src_ref'].to('cuda')\n",
    "        \n",
    "        if args.lang_id_bool:\n",
    "            h = model(x1, langs=seg_x1)[0]\n",
    "        elif args.use_token_type_ids:\n",
    "            h = model(x1, token_type_ids=seg_x1)[1]\n",
    "        else:\n",
    "            h = model(x1)[1]\n",
    "    \n",
    "    labels = batch['label'].to('cuda')\n",
    "    preds = model.mlp(h)\n",
    "    loss = loss_fn(preds.view(-1), labels.view(-1))\n",
    "    \n",
    "    preds = [float(p) for p in preds.view(-1).cpu().detach().numpy()]\n",
    "    labels = [float(t) for t in labels.view(-1).cpu().detach().numpy()]\n",
    "    \n",
    "    if train:\n",
    "        if not args.amp:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with apex.amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return float(loss.item()), preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(model, train_dataloader, mse, optimizer, args, results):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    preds_ls = []\n",
    "    trues_ls = []\n",
    "    raw_srcs = []\n",
    "    raw_refs = []\n",
    "    raw_hyps = []\n",
    "    for n_iter, batch_data in enumerate(train_dataloader):\n",
    "        if args.debug:\n",
    "            print('\\rnumber of iteration = {}'.format(n_iter), end='')\n",
    "        optimizer.zero_grad()\n",
    "        loss, preds, labels = run_model(model, batch_data, args, mse, optimizer, train=True)\n",
    "        losses.append(loss)\n",
    "        preds_ls.extend(preds)\n",
    "        trues_ls.extend(labels)\n",
    "        raw_srcs.extend(batch_data['raw_src'])\n",
    "        raw_refs.extend(batch_data['raw_ref'])\n",
    "        raw_hyps.extend(batch_data['raw_hyp'])\n",
    "\n",
    "    results[args.n_trial-1]['train']['loss'].append(np.mean(losses))\n",
    "    results[args.n_trial-1]['train']['pearson'].append(utils.calc_pearson(preds_ls, trues_ls))\n",
    "    results[args.n_trial-1]['train']['pred'].append(preds_ls)\n",
    "    results[args.n_trial-1]['train']['true'].append(trues_ls)\n",
    "    results[args.n_trial-1]['train']['raw_src'].append(raw_srcs)\n",
    "    results[args.n_trial-1]['train']['raw_ref'].append(raw_refs)\n",
    "    results[args.n_trial-1]['train']['raw_hyp'].append(raw_hyps)\n",
    "    \n",
    "    return model, train_dataloader, mse, optimizer, args, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _valid(model, valid_dataloader, mse, optimizer, args, results, best_val_loss, best_val_pearson):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    preds_ls = []\n",
    "    trues_ls = []\n",
    "    raw_srcs = []\n",
    "    raw_refs = []\n",
    "    raw_hyps = []\n",
    "    for batch_data in valid_dataloader:\n",
    "        with torch.no_grad():\n",
    "            loss, preds, labels = run_model(model, batch_data, args, mse, optimizer)\n",
    "        losses.append(loss)\n",
    "        preds_ls.extend(preds)\n",
    "        trues_ls.extend(labels)\n",
    "        raw_srcs.extend(batch_data['raw_src'])\n",
    "        raw_refs.extend(batch_data['raw_ref'])\n",
    "        raw_hyps.extend(batch_data['raw_hyp'])\n",
    "\n",
    "    results[args.n_trial-1]['valid']['loss'].append(np.mean(losses))\n",
    "    results[args.n_trial-1]['valid']['pearson'].append(utils.calc_pearson(preds_ls, trues_ls))\n",
    "    results[args.n_trial-1]['valid']['pred'].append(preds_ls)\n",
    "    results[args.n_trial-1]['valid']['true'].append(trues_ls)\n",
    "    results[args.n_trial-1]['valid']['raw_src'].append(raw_srcs)\n",
    "    results[args.n_trial-1]['valid']['raw_ref'].append(raw_refs)\n",
    "    results[args.n_trial-1]['valid']['raw_hyp'].append(raw_hyps)\n",
    "\n",
    "    # update lr\n",
    "    if best_val_loss > np.mean(losses):\n",
    "        best_val_loss = np.mean(losses)\n",
    "    else:\n",
    "        optimizer = update_lr(optimizer, args)\n",
    "\n",
    "    # save model\n",
    "#             if best_val_pearson < results[args.n_trial-1]['valid']['pearson'][-1]:\n",
    "#                 best_val_pearson = results[args.n_trial-1]['valid']['pearson'][-1]\n",
    "#                 print('saving a model!')\n",
    "#                 checkpoint = {'model': model.state_dict(),\n",
    "#                               'optimizer': optimizer.state_dict(),\n",
    "#                               'amp': apex.amp.state_dict()}\n",
    "#                 torch.save(checkpoint, os.path.join(args.dump_path,'{}th_best_valid_checkpoint.pth'.format(args.n_trial-1)))\n",
    "#                 print('finished saving!')\n",
    "\n",
    "    return model, valid_dataloader, mse, optimizer, args, results, best_val_loss, best_val_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test(model, test_dataloader, mse, optimizer, args, results, lang_availables):\n",
    "    model.eval()\n",
    "    losses = {lang:[] for lang in args.langs}\n",
    "    losses['all'] = []\n",
    "    preds_ls = {lang:[] for lang in args.langs}\n",
    "    preds_ls['all'] = []\n",
    "    trues_ls = {lang:[] for lang in args.langs}\n",
    "    trues_ls['all'] = []\n",
    "    langs_ls = []\n",
    "    raw_srcs = []\n",
    "    raw_refs = []\n",
    "    raw_hyps = []\n",
    "    for batch_data in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            loss, preds, labels = run_model(model, batch_data, args, mse, optimizer)\n",
    "        langs_ls.extend(batch_data['lang'])\n",
    "        losses['all'].append(loss)\n",
    "        preds_ls['all'].extend(preds)\n",
    "        trues_ls['all'].extend(labels)\n",
    "        raw_srcs.extend(batch_data['raw_src'])\n",
    "        raw_refs.extend(batch_data['raw_ref'])\n",
    "        raw_hyps.extend(batch_data['raw_hyp'])\n",
    "\n",
    "    results[args.n_trial-1]['test']['loss'].append(np.mean(losses['all']))\n",
    "    results[args.n_trial-1]['test']['pearson'].append(utils.calc_pearson(preds_ls['all'], trues_ls['all']))\n",
    "    results[args.n_trial-1]['test']['pred'].append(preds_ls['all'])\n",
    "    results[args.n_trial-1]['test']['true'].append(trues_ls['all'])\n",
    "    results[args.n_trial-1]['test']['raw_src'].append(raw_srcs)\n",
    "    results[args.n_trial-1]['test']['raw_ref'].append(raw_refs)\n",
    "    results[args.n_trial-1]['test']['raw_hyp'].append(raw_hyps)\n",
    "\n",
    "    for lang, pred, true in zip(langs_ls, preds_ls['all'], trues_ls['all']):\n",
    "        preds_ls[lang].append(pred)\n",
    "        trues_ls[lang].append(true)\n",
    "        if lang not in lang_availables:\n",
    "            lang_availables.append(lang)\n",
    "    for lang in lang_availables:\n",
    "        results[args.n_trial-1]['test']['{}_pred'.format(lang)].append(preds_ls[lang])\n",
    "        results[args.n_trial-1]['test']['{}_true'.format(lang)].append(trues_ls[lang])\n",
    "        results[args.n_trial-1]['test']['{}_pearson'.format(lang)].append(utils.calc_pearson(preds_ls[lang], trues_ls[lang]))  \n",
    "        \n",
    "    return model, test_dataloader, mse, optimizer, args, results, lang_availables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @profile\n",
    "def _run_epoch(best_valid_pearsons, best_valid_epochs, lang_availables, \n",
    "               train_dataloader, valid_dataloader, test_dataloader,\n",
    "               args, results, \n",
    "               ModelClass, ConfigClass, \n",
    "               model, optimizer, config, mse):\n",
    "    best_val_pearson = -1.0\n",
    "    best_val_loss = 1000\n",
    "    \n",
    "    for lang in args.langs:\n",
    "    #     results['test']['{}_loss'.format(lang)] = []\n",
    "        results[args.n_trial-1]['test']['{}_pred'.format(lang)] = []\n",
    "        results[args.n_trial-1]['test']['{}_true'.format(lang)] = []\n",
    "        results[args.n_trial-1]['test']['{}_pearson'.format(lang)] = []\n",
    "    for n_epoch in range(args.epoch_size):\n",
    "        start_time = time.time()\n",
    "        # train\n",
    "        if args.train:\n",
    "            model, train_dataloader, mse, optimizer, args, results = _train(model, \n",
    "                                                                            train_dataloader, \n",
    "                                                                            mse, \n",
    "                                                                            optimizer, \n",
    "                                                                            args, \n",
    "                                                                            results)\n",
    "\n",
    "            # valid\n",
    "            model, valid_dataloader, mse, optimizer, args, results, best_val_loss, best_val_pearson = _valid(model, \n",
    "                                                                                                             valid_dataloader, \n",
    "                                                                                                             mse, optimizer, \n",
    "                                                                                                             args, \n",
    "                                                                                                             results, \n",
    "                                                                                                             best_val_loss,\n",
    "                                                                                                             best_val_pearson)\n",
    "\n",
    "        # test\n",
    "        if args.test:\n",
    "            model, test_dataloader, mse, optimizer, args, results, lang_availables =  _test(model, \n",
    "                                                                                            test_dataloader, \n",
    "                                                                                            mse, \n",
    "                                                                                            optimizer, \n",
    "                                                                                            args, \n",
    "                                                                                            results,\n",
    "                                                                                            lang_availables)  \n",
    "        \n",
    "        lang_availables = [l for l in args.langs if l in lang_availables]\n",
    "        end_time = time.time()\n",
    "        print('-----------------')\n",
    "        print('exp_id:{}, n_trial:{}, {}epoch finished!　　Took {}m{}s'.format(args.exp_id, args.n_trial, n_epoch+1, int((end_time-start_time)/60), int((end_time-start_time)%60)))\n",
    "        print('lr = {}'.format(optimizer.param_groups[0]['lr']))\n",
    "        if args.train:\n",
    "            print('train loss_mean:{:.4f}, pearson:{:.4f}'.format(results[args.n_trial-1]['train']['loss'][-1], results[args.n_trial-1]['train']['pearson'][-1]))\n",
    "            print('valid loss_mean:{:.4f}, pearson:{:.4f}'.format(results[args.n_trial-1]['valid']['loss'][-1], results[args.n_trial-1]['valid']['pearson'][-1]))\n",
    "        if args.test:\n",
    "            print('test loss_mean:{:.4f}\\ntest all pearson:{:.4f}'.format(results[args.n_trial-1]['test']['loss'][-1], results[args.n_trial-1]['test']['pearson'][-1]))\n",
    "            for lang in lang_availables:\n",
    "                print('test {} pearson:{:.4f}'.format(lang, results[args.n_trial-1]['test']['{}_pearson'.format(lang)][-1]))\n",
    "\n",
    "        print('-----------------')\n",
    "    \n",
    "    with open(os.path.join(args.dump_path, 'result.pkl'), mode='wb') as w:\n",
    "        pickle.dump(results, w)\n",
    "    \n",
    "    if args.train and args.test:\n",
    "        best_valid_pearson = -1.0\n",
    "        best_valid_epoch = 0\n",
    "        for e, v_pearson in enumerate(results[args.n_trial-1]['valid']['pearson']):\n",
    "            if best_valid_pearson < v_pearson:\n",
    "                best_valid_pearson = v_pearson\n",
    "                best_valid_epoch = e\n",
    "        best_valid_pearsons[args.n_trial-1] = best_valid_pearson\n",
    "        best_valid_epochs[args.n_trial-1] = best_valid_epoch\n",
    "        \n",
    "        print('--- Final Performance of {}th Model (Pearson)---'.format(args.n_trial))\n",
    "        txt = \"\"\n",
    "        for lang in lang_availables:\n",
    "            txt += '{} : {:.3f}'.format(lang, results[args.n_trial-1]['test']['{}_pearson'.format(lang)][best_valid_epoch]) + str(os.linesep)\n",
    "        txt += 'all : {:.3f}'.format(results[args.n_trial-1]['test']['pearson'][best_valid_epoch]) + str(os.linesep)\n",
    "        txt += 'ave : {:.3f}'.format(np.mean([results[args.n_trial-1]['test']['{}_pearson'.format(lang)][best_valid_epoch] for lang in lang_availables])) + str(os.linesep)\n",
    "        print(txt)\n",
    "        performance_summary_filepath = os.path.join(args.dump_path, '{}th_final_pearformance.txt'.format(args.n_trial))\n",
    "        with open(performance_summary_filepath, mode='w', encoding='utf-8') as w:\n",
    "            w.write(txt)\n",
    "            \n",
    "    return (best_valid_pearsons, best_valid_epochs, lang_availables, \n",
    "            results, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TokenizerClass = utils.get_tokenizer_class(args.model_name)\n",
    "ModelClass = utils.get_model_class(args.model_name)\n",
    "ConfigClass = utils.get_config_class(args.model_name)\n",
    "tokenizer = TokenizerClass.from_pretrained(args.model_name)\n",
    "config = ConfigClass.from_pretrained(args.model_name)\n",
    "data_trans = Data_Transformer(args, tokenizer)\n",
    "args.model_config = config\n",
    "DATA = {}\n",
    "if args.train:\n",
    "    DATA['train'] = Dataset(data_trans, tokenizer, args.data_paths_train, args, '{}.train'.format(args.exp_name))\n",
    "    DATA['valid'] = Dataset(data_trans, tokenizer, args.data_paths_valid, args, '{}.valid'.format(args.exp_name))\n",
    "if args.test:\n",
    "    DATA['test'] = Dataset(data_trans, tokenizer, args.data_paths_test, args, '{}.test'.format(args.exp_name))\n",
    "train_dataloader = torch.utils.data.DataLoader(DATA['train'], \n",
    "                                               batch_size=args.batch_size, \n",
    "                                               collate_fn=data_trans.collate_fn, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(DATA['valid'], batch_size=args.batch_size,\n",
    "                                               collate_fn=data_trans.collate_fn, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(DATA['test'], batch_size=args.batch_size,\n",
    "                                              collate_fn=data_trans.collate_fn, shuffle=False)\n",
    "\n",
    "model = ModelClass.from_pretrained(args.model_name, config=config)\n",
    "model.config.num_labels = 1\n",
    "\n",
    "if args.hyp_src_hyp_ref:\n",
    "    model.mlp = nn.Sequential(*[nn.Dropout(args.dropout),nn.Linear(model.config.hidden_size*2, 1)])\n",
    "else:\n",
    "    model.mlp = nn.Sequential(*[nn.Dropout(args.dropout),nn.Linear(model.config.hidden_size, 1)])\n",
    "\n",
    "optimizer = utils.get_optimizer(list(model.parameters()), args.optimizer)\n",
    "mse = nn.MSELoss()\n",
    "model.to('cuda')\n",
    "\n",
    "if args.amp:\n",
    "    model, optimizer = apex.amp.initialize(\n",
    "        model,\n",
    "        optimizer,\n",
    "        opt_level=('O%i' % 1)\n",
    "    )\n",
    "\n",
    "result_path = os.path.join(args.dump_path, 'result.pkl')\n",
    "if not os.path.isfile(result_path):\n",
    "    results = [{mode:{key:[] for key in ['loss', 'pearson', 'pred', 'true', 'raw_src', 'raw_ref', 'raw_hyp']} for mode in ['train', 'valid', 'test']} for _ in range(args.trial_times)]\n",
    "else:\n",
    "    with open(result_path, mode='rb') as r:\n",
    "        results = pickle.load(r) \n",
    "if len(results[args.n_trial-1]['valid']['pearson']) > args.epoch_size:\n",
    "    for _ in range(len(results[args.n_trial-1]['valid']['pearson']) - args.epoch_size):\n",
    "        for key in results[args.n_trial-1]['valid'].keys():\n",
    "            if type(results[args.n_trial-1]['valid'][key]) == list and len(results[args.n_trial-1]['valid'][key]) != 0:\n",
    "                results[args.n_trial-1]['valid'][key].pop(-1)\n",
    "            if type(results[args.n_trial-1]['train'][key]) == list and len(results[args.n_trial-1]['train'][key]) != 0:\n",
    "                results[args.n_trial-1]['train'][key].pop(-1)\n",
    "if len(results[args.n_trial-1]['test']['pearson']) > args.epoch_size:\n",
    "    for _ in range(len(results[args.n_trial-1]['test']['pearson']) - args.epoch_size):\n",
    "        for key in results[args.n_trial-1]['test'].keys():\n",
    "            if type(results[args.n_trial-1]['test'][key]) == list and len(results[args.n_trial-1]['test'][key]) != 0:\n",
    "                results[args.n_trial-1]['test'][key].pop(-1)\n",
    "\n",
    "best_valid_pearsons_path = os.path.join(args.dump_path, 'best_valid_pearsons.pkl')\n",
    "if not os.path.isfile(best_valid_pearsons_path):\n",
    "    best_valid_pearsons = [-1.0]*args.trial_times\n",
    "else:\n",
    "    with open(best_valid_pearsons_path, mode='rb') as r:\n",
    "        best_valid_pearsons = pickle.load(r) \n",
    "\n",
    "best_valid_epochs_path = os.path.join(args.dump_path, 'best_valid_epochs.pkl')\n",
    "if not os.path.isfile(best_valid_epochs_path):\n",
    "    best_valid_epochs = [0]*args.trial_times\n",
    "else:\n",
    "    with open(best_valid_epochs_path, mode='rb') as r:\n",
    "        best_valid_epochs = pickle.load(r) \n",
    "\n",
    "lang_availables = [] # only for test\n",
    "if args.train or args.test:\n",
    "    if len(results[args.n_trial-1]['valid']['pearson']) != args.epoch_size or len(results[args.n_trial-1]['test']['pearson']) != args.epoch_size:\n",
    "        _run_epoch(best_valid_pearsons, best_valid_epochs, lang_availables, \n",
    "                   train_dataloader, valid_dataloader, test_dataloader,\n",
    "                   args, results, \n",
    "                   ModelClass, ConfigClass, \n",
    "                   model, optimizer, config, mse)\n",
    "    \n",
    "if args.train and args.test and args.n_trial == args.trial_times:\n",
    "    best_val_r = -1.0\n",
    "    best_trial = 0\n",
    "    for n_t, r in enumerate(best_valid_pearsons):\n",
    "        if best_val_r < r:\n",
    "            best_val_r = r\n",
    "            best_trial = n_t\n",
    "    txt = \"\"\n",
    "    txt += '--- Best Model : {}th Model---'.format(best_trial+1) + str(os.linesep)\n",
    "    e = best_valid_epochs[best_trial]\n",
    "    for lang in lang_availables:\n",
    "        txt += '{} : {:.3f}'.format(lang, results[best_trial]['test']['{}_pearson'.format(lang)][e]) + str(os.linesep)\n",
    "    txt += 'all : {:.3f}'.format(results[best_trial]['test']['pearson'][e]) + str(os.linesep)\n",
    "    txt += 'ave : {:.3f}'.format(np.mean([results[best_trial]['test']['{}_pearson'.format(lang)][e] for lang in lang_availables])) + str(os.linesep)\n",
    "    print(txt)\n",
    "    summary_filepath = os.path.join(args.dump_path, 'final_result.txt')\n",
    "    with open(summary_filepath, mode='w', encoding='utf-8') as w:\n",
    "        w.write(txt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bash run.sh --exp_name multiBERT_all_hyp_ref --optimizer adam,lr=0.00003 --batch_size 16 --epoch_size 20 --hyp_src True --hyp_src_hyp_ref False\n",
    "\n",
    "# bash run.sh --exp_name xlm-r-large_hyp_ref --hyp_ref True --model_name xlm-roberta-large --optimizer adam,lr=0.000007 adam,lr=0.000006 adam,lr=0.000005 adam,lr=0.000004 --epoch_size 10 --batch_size 2 --trial_times 10 --langs cs-en,de-en,lv-en,fi-en,ro-en,ru-en,tr-en,zh-en\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
