{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.stats import pearsonr as pr\n",
    "from scipy.stats import spearmanr as sr\n",
    "import copy\n",
    "import pandas as pd\n",
    "import difflib\n",
    "from transformers import *\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import optim\n",
    "from typing import Tuple\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn\n",
    "\n",
    "from shutil import rmtree\n",
    "\n",
    "import logging\n",
    "import utils\n",
    "random.seed(77)\n",
    "# torch.manual_seed(77)\n",
    "# np.random.seed(0)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset \n",
    "must be build independently from each mode such as 'train', 'dev', 'test:de', test:cs'\n",
    "\n",
    "preprocessing:\n",
    "    read data from data_paths\n",
    "    tokenize data\n",
    "    make lang_token\n",
    "    uniform length of mini-batch data and insert pad token\n",
    "    \n",
    "\"\"\"\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform, tokenizer, data_paths, args, data_name=None, test=False):\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_paths = data_paths\n",
    "        self.args = args\n",
    "        self.test = test\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        self.savedata_dir = os.path.join(args.tmp_path, '{}.pkl'.format(data_name))\n",
    "        if not os.path.isfile(self.savedata_dir):\n",
    "            self.data = self.read_data(self.data_paths, tokenizer)\n",
    "            with open(self.savedata_dir, mode='wb') as w:\n",
    "                pickle.dump(self.data, w)\n",
    "        else:\n",
    "            with open(self.savedata_dir, mode='rb') as r:\n",
    "                self.data = pickle.load(r)\n",
    "                \n",
    "        self.limit_lang()\n",
    "        if self.args.train_shrink < 1.0:\n",
    "            self.data = random.sample(self.data, int(len(self.data)/2))\n",
    "    \n",
    "    def limit_lang(self):\n",
    "        data_list = []\n",
    "        for data in self.data:\n",
    "            if data['lang'] in self.args.langs:\n",
    "                data_list.append(data)\n",
    "        self.data = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            out_data = self.transform(out_data)\n",
    "\n",
    "        return out_data\n",
    "    \n",
    "    def encode2sents(self, sent1, sent2, tokenizer):\n",
    "        return tokenizer.encode(sent1, sent2)\n",
    "        \n",
    "    def encode3sents(self, sent1, sent2, sent3, tokenizer, bos_id, sep_id, eos_id, insert_bos=False):\n",
    "        x = tokenizer.encode(sent1, sent2)\n",
    "        if not insert_bos:\n",
    "            x[-1] = sep_id\n",
    "        else:\n",
    "            x.append(bos_id)\n",
    "        sent3_ids = tokenizer.encode(sent3)\n",
    "        sent3_ids.pop(0)\n",
    "        x.extend(sent3_ids)\n",
    "        return x\n",
    "    \n",
    "    def get_seqment_id(self, tokens, bos_id, sep_id, eos_id):\n",
    "        x = []\n",
    "        index = 0\n",
    "        for idx, tok in enumerate(tokens):\n",
    "            if tok in [sep_id, eos_id]:\n",
    "                index = idx\n",
    "                break\n",
    "        x += [0] * (index+1)\n",
    "        x += [1] * (len(tokens)-len(x))\n",
    "        return x\n",
    "    \n",
    "    def get_lang_id(self, lang_pair, tokens, sep_id, eos_id, use_src=True, hyp_src_ref=False):\n",
    "        lang1_id = self.tokenizer.lang2id[lang_pair.split('-')[0]]\n",
    "        lang2_id = self.tokenizer.lang2id[lang_pair.split('-')[1]]\n",
    "        x = []\n",
    "        index = 0\n",
    "        index2 = 0\n",
    "        for idx, tok in enumerate(tokens):\n",
    "            if tok in [sep_id, eos_id] and idx != 0:\n",
    "                if index == 0:\n",
    "                    index = idx\n",
    "                else:\n",
    "                    index2 = idx\n",
    "                if (not hyp_src_ref) or index2 != 0:\n",
    "                    break\n",
    "                \n",
    "        x += [lang2_id] * (index+1)\n",
    "        if not hyp_src_ref:\n",
    "            if use_src:\n",
    "                x += [lang1_id] * (len(tokens)-len(x))\n",
    "            else:\n",
    "                x += [lang2_id] * (len(tokens)-len(x))\n",
    "        \n",
    "        else:\n",
    "            x += [lang1_id] * (index2-(index+1))\n",
    "            x += [lang2_id] * (len(tokens)-len(x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    ### add tokenizing process\n",
    "    def read_data(self, data_paths, tokenizer):\n",
    "        \"\"\"\n",
    "        data format must be\n",
    "        \n",
    "        {data}\\t{lang}\\n \n",
    "        \n",
    "        \n",
    "        return data:\n",
    "        {\n",
    "         'raw_src':txt, \n",
    "         'raw_ref':txt, \n",
    "         'raw_hyp':txt, \n",
    "         'label':float,\n",
    "         'lang':language pair\n",
    "         'tok_hyp_src': ,\n",
    "         'tok_hyp_src_ref',\n",
    "         'tok_hyp_ref':\n",
    "         'seg_hyp_src':,\n",
    "         'seg_hyp_src_ref',\n",
    "         'seg_hyp_ref':\n",
    "         }\n",
    "        \n",
    "        \"\"\"\n",
    "        forms = ['src', 'ref', 'hyp', 'label']\n",
    "        DATA = {form:None for form in forms}\n",
    "        if tokenizer.bos_token_id != None:\n",
    "            bos_id = tokenizer.bos_token_id\n",
    "        else:\n",
    "            bos_id = tokenizer.cls_token_id\n",
    "        if tokenizer.eos_token_id != None:\n",
    "            eos_id = tokenizer.eos_token_id\n",
    "        else:\n",
    "            eos_id = tokenizer.sep_token_id\n",
    "        if tokenizer.sep_token_id != None:\n",
    "            sep_id = tokenizer.sep_token_id\n",
    "        else:\n",
    "            sep_id = tokenizer.eos_token_id\n",
    "        \n",
    "        for data_path, form in zip(data_paths, forms):\n",
    "            assert os.path.isfile(data_path)\n",
    "            with open(data_path, mode='r', encoding='utf-8') as r:\n",
    "                data = r.read().split(os.linesep)\n",
    "                if data[-1] == '':\n",
    "                    data.pop(-1)\n",
    "            DATA[form] = data\n",
    "        r_data = []\n",
    "        for i in range(len(DATA[forms[0]])):\n",
    "            tmp_dic = {}\n",
    "            if DATA[forms[0]][i].split('\\t')[1] not in self.args.langs:\n",
    "                continue\n",
    "            for form in forms:\n",
    "                d = DATA[form][i]\n",
    "                lang = d.split('\\t')[1]\n",
    "                if form == 'label':\n",
    "                    if self.test and self.args.darr:\n",
    "                        tmp_dic[form] = str(d.split('\\t')[0])\n",
    "                    else:\n",
    "                        tmp_dic[form] = float(d.split('\\t')[0])\n",
    "                else:\n",
    "                    tmp_dic['raw_{}'.format(form)] = d.split('\\t')[0]\n",
    "                if 'lang' in tmp_dic and form != 'label':\n",
    "#                     if not tmp_dic['lang'] == d.split('\\t')[1]:\n",
    "#                         import pdb;pdb.set_trace()\n",
    "                    assert tmp_dic['lang'] == lang\n",
    "                else:\n",
    "                    tmp_dic['lang'] = lang\n",
    "            if 'src' in self.args.forms:\n",
    "                tmp_dic['tok_hyp_src'] = self.encode2sents(tmp_dic['raw_hyp'], tmp_dic['raw_src'], tokenizer)\n",
    "                if len(tmp_dic['tok_hyp_src']) >= tokenizer.model_max_length:\n",
    "                    tmp_dic['tok_hyp_src'] = self.encode2sents(tmp_dic['raw_hyp'][:int(len(tmp_dic['raw_hyp'])*0.85)], tmp_dic['raw_src'], tokenizer)\n",
    "                    if len(tmp_dic['tok_hyp_src']) >= tokenizer.model_max_length:\n",
    "                        tmp_dic['tok_hyp_src'] = self.encode2sents(tmp_dic['raw_hyp'][:int(len(tmp_dic['raw_hyp'])*0.75)], tmp_dic['raw_src'], tokenizer)\n",
    "                        if len(tmp_dic['tok_hyp_src']) >= tokenizer.model_max_length:\n",
    "                            tmp_dic['tok_hyp_src'] = self.encode2sents(tmp_dic['raw_hyp'][:int(len(tmp_dic['raw_hyp'])*0.65)], tmp_dic['raw_src'], tokenizer)\n",
    "                    if len(tmp_dic['tok_hyp_src']) >= tokenizer.model_max_length:\n",
    "                        self.args.logger.error('seqence token length ({}) is over model_max_length ({})'.format(len(tmp_dic['tok_hyp_src']), tokenizer.model_max_length))\n",
    "                        #import pdb;pdb.set_trace()\n",
    "                tmp_dic['seg_hyp_src'] = self.get_seqment_id(tmp_dic['tok_hyp_src'], bos_id, sep_id, eos_id)\n",
    "                if self.args.lang_id_bool:\n",
    "                    tmp_dic['lang_hyp_src'] = self.get_lang_id(tmp_dic['lang'], tmp_dic['tok_hyp_src'], sep_id, eos_id)\n",
    "                \n",
    "                if 'ref' in self.args.forms:\n",
    "                    tmp_dic['tok_hyp_src_ref'] = self.encode3sents(tmp_dic['raw_hyp'], \n",
    "                                                                   tmp_dic['raw_src'], \n",
    "                                                                   tmp_dic['raw_ref'], \n",
    "                                                                   tokenizer, bos_id, sep_id, eos_id)\n",
    "                    if len(tmp_dic['tok_hyp_src_ref']) > tokenizer.model_max_length:\n",
    "                        tmp_dic['tok_hyp_src_ref'] = self.encode3sents(tmp_dic['raw_hyp'][:int(len(tmp_dic['raw_hyp'])*0.85)], \n",
    "                                                                   tmp_dic['raw_src'][:int(len(tmp_dic['raw_src'])*0.85)], \n",
    "                                                                   tmp_dic['raw_ref'][:int(len(tmp_dic['raw_ref'])*0.85)], \n",
    "                                                                   tokenizer, bos_id, sep_id, eos_id)\n",
    "                        if len(tmp_dic['tok_hyp_src_ref']) > tokenizer.model_max_length:\n",
    "                            tmp_dic['tok_hyp_src_ref'] = self.encode3sents(tmp_dic['raw_hyp'][:int(len(tmp_dic['raw_hyp'])*0.70)], \n",
    "                                                                       tmp_dic['raw_src'][:int(len(tmp_dic['raw_src'])*0.70)], \n",
    "                                                                       tmp_dic['raw_ref'][:int(len(tmp_dic['raw_ref'])*0.70)], \n",
    "                                                                       tokenizer, bos_id, sep_id, eos_id)\n",
    "                            if len(tmp_dic['tok_hyp_src_ref']) > tokenizer.model_max_length:\n",
    "                                tmp_dic['tok_hyp_src_ref'] = self.encode3sents(tmp_dic['raw_hyp'][:int(len(tmp_dic['raw_hyp'])*0.50)], \n",
    "                                                                           tmp_dic['raw_src'][:int(len(tmp_dic['raw_src'])*0.50)], \n",
    "                                                                           tmp_dic['raw_ref'][:int(len(tmp_dic['raw_ref'])*0.50)], \n",
    "                                                                           tokenizer, bos_id, sep_id, eos_id)\n",
    "                    tmp_dic['seg_hyp_src_ref'] = self.get_seqment_id(tmp_dic['tok_hyp_src_ref'], bos_id, sep_id, eos_id)\n",
    "                    if self.args.lang_id_bool:\n",
    "                        tmp_dic['lang_hyp_src_ref'] = self.get_lang_id(tmp_dic['lang'], tmp_dic['tok_hyp_src_ref'], sep_id, eos_id, hyp_src_ref=True)\n",
    "            if 'ref' in self.args.forms:\n",
    "                tmp_dic['tok_hyp_ref'] = self.encode2sents(tmp_dic['raw_hyp'], tmp_dic['raw_ref'], tokenizer)\n",
    "                if len(tmp_dic['tok_hyp_ref']) >= tokenizer.model_max_length:\n",
    "                    tmp_dic['tok_hyp_ref'] = self.encode2sents(tmp_dic['raw_hyp'][:int(len(tmp_dic['raw_hyp'])*0.85)], tmp_dic['raw_ref'], tokenizer)\n",
    "                    if len(tmp_dic['tok_hyp_ref']) >= tokenizer.model_max_length:\n",
    "                        tmp_dic['tok_hyp_ref'] = self.encode2sents(tmp_dic['raw_hyp'][:int(len(tmp_dic['raw_hyp'])*0.75)], tmp_dic['raw_ref'], tokenizer)\n",
    "                        if len(tmp_dic['tok_hyp_ref']) >= tokenizer.model_max_length:\n",
    "                            tmp_dic['tok_hyp_ref'] = self.encode2sents(tmp_dic['raw_hyp'][:int(len(tmp_dic['raw_hyp'])*0.65)], tmp_dic['raw_ref'], tokenizer)\n",
    "                    if len(tmp_dic['tok_hyp_ref']) >= tokenizer.model_max_length:\n",
    "                        self.args.logger.error('seqence token length ({}) is over model_max_length ({})'.format(len(tmp_dic['tok_hyp_ref']), tokenizer.model_max_length))\n",
    "                        #import pdb;pdb.set_trace()\n",
    "                tmp_dic['seg_hyp_ref'] = self.get_seqment_id(tmp_dic['tok_hyp_ref'], bos_id, sep_id, eos_id)\n",
    "                if self.args.lang_id_bool:\n",
    "                    tmp_dic['lang_hyp_ref'] = self.get_lang_id(tmp_dic['lang'], tmp_dic['tok_hyp_ref'], sep_id, eos_id, use_src=False)\n",
    "                    \n",
    "            r_data.append(tmp_dic)\n",
    "        return r_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Transformer():\n",
    "    \"\"\"\n",
    "    batch : type == dict\n",
    "    batch : \n",
    "    {\n",
    "      'raw_src': [~]\n",
    "      'tok_src': [~]\n",
    "      'raw_ref': [~]\n",
    "      ....\n",
    "      'raw_label': [~]\n",
    "      'tok_label': [~]\n",
    "      'lang':language pair\n",
    "    }\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, args, tokenizer, test=False):\n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "        self.test = test\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "#         import pdb;pdb.set_trace()\n",
    "        return batch\n",
    "\n",
    "\n",
    "    def padding(self, tok_list, pad_id=None, lang_padding=False):\n",
    "        args = self.args\n",
    "        if utils.get_model_type(args.model_name) == 'reformer':\n",
    "            max_seq_len = max([len(x) for x in tok_list])\n",
    "            if not max_seq_len % args.model_config.lsh_attn_chunk_length == 0:\n",
    "                max_seq_len = (int(max_seq_len / args.model_config.lsh_attn_chunk_length) + 1) *  args.model_config.lsh_attn_chunk_length\n",
    "        else:\n",
    "            max_seq_len = max([len(x) for x in tok_list])\n",
    "        bs_size = len(tok_list)\n",
    "        new_tok_list = []\n",
    "        for toks in tok_list:\n",
    "            if pad_id == None:\n",
    "                if lang_padding:\n",
    "                    toks += [toks[-1]]*(max_seq_len-len(toks))\n",
    "                else:\n",
    "                    toks += [self.pad_id]*(max_seq_len-len(toks))   \n",
    "            else:\n",
    "                toks += [pad_id]*(max_seq_len-len(toks))\n",
    "            new_tok_list.append(toks)\n",
    "        x = torch.tensor(new_tok_list)\n",
    "        return x\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        tok_hyp_src = []\n",
    "        tok_hyp_ref = []\n",
    "        tok_hyp_src_ref = []\n",
    "        seg_hyp_src = []\n",
    "        seg_hyp_ref = []\n",
    "        seg_hyp_src_ref = []\n",
    "        lang_hyp_src = []\n",
    "        lang_hyp_ref = []\n",
    "        lang_hyp_src_ref = []\n",
    "        return_dic = {'raw_src':[], \n",
    "                      'raw_ref':[], \n",
    "                      'raw_hyp':[], \n",
    "                      'label':[], \n",
    "                      'lang':[]\n",
    "                     }\n",
    "        for btch in batch:\n",
    "            return_dic['raw_src'].append(btch['raw_src'])\n",
    "            return_dic['raw_ref'].append(btch['raw_ref'])\n",
    "            return_dic['raw_hyp'].append(btch['raw_hyp'])\n",
    "            if self.args.darr and self.test:\n",
    "                return_dic['label'].append(btch['label'])\n",
    "            else:\n",
    "                return_dic['label'].append(float(btch['label']))\n",
    "            if 'src' in self.args.forms:\n",
    "                tok_hyp_src.append(btch['tok_hyp_src'])\n",
    "                seg_hyp_src.append(btch['seg_hyp_src'])\n",
    "                if self.args.lang_id_bool:\n",
    "                    lang_hyp_src.append(btch['lang_hyp_src'])\n",
    "                if 'ref' in self.args.forms:\n",
    "                    tok_hyp_src_ref.append(btch['tok_hyp_src_ref'])\n",
    "                    seg_hyp_src_ref.append(btch['seg_hyp_src_ref'])\n",
    "                    if self.args.lang_id_bool:\n",
    "                        lang_hyp_src_ref.append(btch['lang_hyp_src_ref'])\n",
    "            if 'ref' in self.args.forms:\n",
    "                tok_hyp_ref.append(btch['tok_hyp_ref'])\n",
    "                seg_hyp_ref.append(btch['seg_hyp_ref'])\n",
    "                if self.args.lang_id_bool:\n",
    "                    lang_hyp_ref.append(btch['lang_hyp_ref'])\n",
    "            return_dic['lang'].append(btch['lang'])\n",
    "        \n",
    "        if 'src' in self.args.forms:\n",
    "            return_dic['hyp_src'] = self.padding(tok_hyp_src)\n",
    "            return_dic['seg_hyp_src'] = self.padding(seg_hyp_src, pad_id=1)\n",
    "            if self.args.lang_id_bool:\n",
    "                return_dic['lang_hyp_src'] = self.padding(lang_hyp_src, lang_padding=True)\n",
    "            if 'ref' in self.args.forms:\n",
    "                return_dic['hyp_src_ref'] = self.padding(tok_hyp_src_ref)\n",
    "                return_dic['seg_hyp_src_ref'] = self.padding(seg_hyp_src_ref, pad_id=1)\n",
    "                if self.args.lang_id_bool:\n",
    "                    return_dic['lang_hyp_src_ref'] = self.padding(lang_hyp_src_ref, lang_padding=True)\n",
    "        if 'ref' in self.args.forms:\n",
    "            return_dic['hyp_ref'] = self.padding(tok_hyp_ref)\n",
    "            return_dic['seg_hyp_ref'] = self.padding(seg_hyp_ref, pad_id=1)\n",
    "            if self.args.lang_id_bool:\n",
    "                return_dic['lang_hyp_ref'] = self.padding(lang_hyp_ref, lang_padding=True)\n",
    "        \n",
    "        if self.args.darr and self.test:\n",
    "            return_dic['label'] = torch.FloatTensor([0.0] * len(return_dic['label']))\n",
    "        else:\n",
    "            return_dic['label'] = torch.FloatTensor(return_dic['label'])\n",
    "    \n",
    "        return return_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# a = torch.ones(1, 5)\n",
    "# b = torch.ones(1, 8)\n",
    "# print(a)\n",
    "# print(b)\n",
    "\n",
    "# torch.nn.utils.rnn.pad_sequence([b, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
    "# import torch\n",
    "\n",
    "# model_name = 'xlm-roberta-large'\n",
    "# tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "# model = XLMRobertaModel.from_pretrained(model_name)\n",
    "\n",
    "# input_ids = torch.load('/ahc/work3/kosuke-t/SRHDA/transformers/log/xlm-r-large_hyp_src_ref/1/debug_data.pth')  # Batch size 1\n",
    "# outputs = model(input_ids)\n",
    "# sentvec = outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 567])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 20920, 6, 127, 2335, 16, 11962, 2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
